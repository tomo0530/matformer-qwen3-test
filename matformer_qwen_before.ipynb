{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatFormer for Qwen3\n",
    "\n",
    "## Model Specifications\n",
    "- **Qwen3-4B**: 4.0B parameters, 36 layers, 32 Q heads, 8 KV heads\n",
    "- **Qwen3-1.7B**: 1.7B parameters, 28 layers, 16 Q heads, 8 KV heads\n",
    "- **Target 3B**: Custom configuration with Mix-n-Match FFN dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¡ãƒ¢ãƒªç®¡ç†ã®æœ€é©åŒ–\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import traceback\n",
    "from huggingface_hub import snapshot_download, HfApi\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_4b_id = \"Qwen/Qwen3-4B\"\n",
    "model_1_7b_id = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# Output configuration\n",
    "local_output_path = \"./output/qwen3_3b_model\"\n",
    "funetuning_output_path = \"./output/matformer_finetune_results_integrated\"\n",
    "final_output_path = \"./output/matformer_qwen3_3b_finetuned\"\n",
    "\n",
    "# Target 3B model configuration\n",
    "target_num_layers = 32\n",
    "target_params = \"3B\"\n",
    "\n",
    "def best_gpu(exclude=(0,)):\n",
    "    \"\"\"ç©ºããƒ¡ãƒ¢ãƒªãŒæœ€å¤§ã® GPU ã‚’è¿”ã™ï¼ˆexclude ã§é™¤å¤–ç•ªå·ã‚’æ¸¡ã›ã‚‹ï¼‰\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"cpu\"\n",
    "    best, max_free = None, -1\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        if i in exclude:\n",
    "            continue\n",
    "        free, _ = torch.cuda.mem_get_info(i)\n",
    "        if free > max_free:\n",
    "            best, max_free = i, free\n",
    "    return f\"cuda:{best}\" if best is not None else \"cpu\"\n",
    "\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆGPUã‚’è¨­å®š\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "# GPU0 ã‚’é™¤å¤–ã—ã¦é¸æŠ\n",
    "device = best_gpu(exclude=(0,))\n",
    "torch.cuda.set_device(int(device.split(\":\")[-1]) if device.startswith(\"cuda\") else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MatFormer Custom Architecture Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatQwenMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3ã®FFN(MLP)ã‚’MatFormerã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«å¤‰æ›´ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ©ã‚¹ã€‚\n",
    "    è¨“ç·´ã¨æ¨è«–ã®ä¸¡æ–¹ã§å‹•çš„ã«ã‚µã‚¤ã‚ºã‚’å¤‰æ›´ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    def __init__(self, config, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, ffn_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, ffn_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(ffn_dim, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "        # MatFormerç”¨ã®è¨­å®š\n",
    "        self.full_intermediate_size = ffn_dim\n",
    "        self.current_intermediate_size = ffn_dim # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ•ãƒ«ã‚µã‚¤ã‚º\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # ç¾åœ¨è¨­å®šã•ã‚Œã¦ã„ã‚‹ã‚µã‚¤ã‚ºã§é‡ã¿ã‚’ã‚¹ãƒ©ã‚¤ã‚¹ã—ã¦è¨ˆç®—\n",
    "        active_gate_weight = self.gate_proj.weight[:self.current_intermediate_size, :]\n",
    "        active_up_weight = self.up_proj.weight[:self.current_intermediate_size, :]\n",
    "        active_down_weight = self.down_proj.weight[:, :self.current_intermediate_size]\n",
    "\n",
    "        # å…¥åŠ›ã¨åŒã˜ãƒ‡ãƒ¼ã‚¿å‹ã«å¤‰æ›\n",
    "        if x.dtype != active_gate_weight.dtype:\n",
    "            active_gate_weight = active_gate_weight.to(x.dtype)\n",
    "            active_up_weight = active_up_weight.to(x.dtype)\n",
    "            active_down_weight = active_down_weight.to(x.dtype)\n",
    "\n",
    "        gate_output = nn.functional.linear(x, active_gate_weight)\n",
    "        up_output = nn.functional.linear(x, active_up_weight)\n",
    "\n",
    "        activated_output = self.act_fn(gate_output) * up_output\n",
    "\n",
    "        # transposeãƒ•ãƒ©ã‚°ã‚’Falseã«è¨­å®šã—ã¦åŠ¹ç‡åŒ–\n",
    "        output = nn.functional.linear(activated_output, active_down_weight, bias=None)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations for both models\n",
    "config_4b = AutoConfig.from_pretrained(model_4b_id)\n",
    "config_1_7b = AutoConfig.from_pretrained(model_1_7b_id)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_4b_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Target 3B Model Configuration with Mix-n-Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target 3B configuration based on 4B model with Mix-n-Match approach\n",
    "target_config = copy.deepcopy(config_4b)\n",
    "\n",
    "# Target configuration for 3B model with Mix-n-Match\n",
    "target_config.num_hidden_layers = 32  # Between 4B(36) and 1.7B(28)\n",
    "\n",
    "# Mix-n-Match: Use different FFN dimensions for different layers\n",
    "# Inspired by Gemma3n's approach - later layers get more capacity\n",
    "ffn_dims_per_layer = []\n",
    "for i in range(32):\n",
    "    if i < 8:  # Early layers: smallest FFN (similar to 1.7B)\n",
    "        ffn_dims_per_layer.append(8192)\n",
    "    elif i < 16:  # Early-middle layers: 1.7B size\n",
    "        ffn_dims_per_layer.append(9728)\n",
    "    elif i < 24:  # Late-middle layers: between 1.7B and 4B\n",
    "        ffn_dims_per_layer.append(11776)\n",
    "    else:  # Final layers: closer to 4B size\n",
    "        ffn_dims_per_layer.append(13312)\n",
    "\n",
    "# For Qwen3, we need to set a single intermediate_size in config\n",
    "# We'll use the maximum size to ensure compatibility\n",
    "target_config.intermediate_size = max(ffn_dims_per_layer)\n",
    "target_config.num_attention_heads = 32  # Keep same as 4B\n",
    "target_config.num_key_value_heads = 8   # Keep same as both models\n",
    "\n",
    "# Store the per-layer FFN dimensions as a custom attribute\n",
    "target_config.ffn_dims_per_layer = ffn_dims_per_layer\n",
    "\n",
    "# Estimate parameter count with Mix-n-Match\n",
    "def estimate_params_mixnmatch(config, ffn_dims):\n",
    "    hidden_size = config.hidden_size\n",
    "    num_layers = config.num_hidden_layers\n",
    "    vocab_size = config.vocab_size\n",
    "    \n",
    "    # Embedding parameters\n",
    "    embed_params = vocab_size * hidden_size\n",
    "    \n",
    "    # Attention parameters per layer (same for all layers)\n",
    "    attn_params_per_layer = (\n",
    "        hidden_size * hidden_size * 3 +  # q, k, v projections\n",
    "        hidden_size * hidden_size        # output projection\n",
    "    )\n",
    "    \n",
    "    # Layer norm parameters per layer\n",
    "    ln_params_per_layer = hidden_size * 2  # input and post-attention layer norms\n",
    "    \n",
    "    # Calculate FFN parameters for each layer individually\n",
    "    total_ffn_params = 0\n",
    "    for layer_idx in range(num_layers):\n",
    "        intermediate_size = ffn_dims[layer_idx]\n",
    "        ffn_params = (\n",
    "            hidden_size * intermediate_size * 2 +  # gate and up projections\n",
    "            intermediate_size * hidden_size         # down projection\n",
    "        )\n",
    "        total_ffn_params += ffn_params\n",
    "    \n",
    "    # Total transformer parameters\n",
    "    transformer_params = num_layers * (attn_params_per_layer + ln_params_per_layer) + total_ffn_params\n",
    "    \n",
    "    # Output layer parameters\n",
    "    output_params = vocab_size * hidden_size\n",
    "    \n",
    "    total_params = embed_params + transformer_params + output_params\n",
    "    return total_params\n",
    "\n",
    "estimated_params = estimate_params_mixnmatch(target_config, ffn_dims_per_layer)\n",
    "\n",
    "# Calculate average FFN dimension for reference\n",
    "avg_ffn_dim = sum(ffn_dims_per_layer) / len(ffn_dims_per_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 19660.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download model checkpoints\n",
    "model_4b_path = snapshot_download(model_4b_id, allow_patterns=[\"*.safetensors\"])\n",
    "safetensor_4b_files = [os.path.join(model_4b_path, f) for f in os.listdir(model_4b_path) if f.endswith('.safetensors')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MatFormer Implementation: Create 3B Model with Mix-n-Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matformer_3b_model():\n",
    "    # Create output directory\n",
    "    os.makedirs(local_output_path, exist_ok=True)\n",
    "    \n",
    "    # Save target configuration and tokenizer\n",
    "    target_config.save_pretrained(local_output_path)\n",
    "    tokenizer.save_pretrained(local_output_path)\n",
    "    \n",
    "    # Layer mapping strategy\n",
    "    source_layers = list(range(32))  # Use first 32 layers from 4B model\n",
    "    target_layers = list(range(32))  # Map to 32 target layers\n",
    "    layer_mapping = {src: tgt for src, tgt in zip(source_layers, target_layers)}\n",
    "    \n",
    "    return layer_mapping\n",
    "\n",
    "layer_mapping = create_matformer_3b_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4B model shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:09<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process model weights and create 3B model with Mix-n-Match\n",
    "def process_model_weights():\n",
    "    # Weight mapping for the new model\n",
    "    weight_map = {}\n",
    "    new_shard_state_dict = {}\n",
    "    shard_counter = 1\n",
    "    max_shard_size = 4 * 1024 * 1024 * 1024  # 4GB per shard\n",
    "    \n",
    "    # Get per-layer FFN dimensions\n",
    "    if hasattr(target_config, 'ffn_dims_per_layer'):\n",
    "        ffn_dims_per_layer = target_config.ffn_dims_per_layer\n",
    "    else:\n",
    "        # Fallback to single dimension\n",
    "        ffn_dims_per_layer = [target_config.intermediate_size] * target_config.num_hidden_layers\n",
    "        print(f\"Using uniform FFN dimension: {target_config.intermediate_size}\")\n",
    "    \n",
    "    source_intermediate_size = config_4b.intermediate_size\n",
    "    \n",
    "    pbar = tqdm(total=len(safetensor_4b_files), desc=\"Processing 4B model shards\")\n",
    "    \n",
    "    for shard_path in safetensor_4b_files:\n",
    "        with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for tensor_name in f.keys():\n",
    "                tensor = f.get_tensor(tensor_name)\n",
    "                new_tensor_name = tensor_name\n",
    "                \n",
    "                # Handle layer-specific parameters\n",
    "                layer_match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n",
    "                if layer_match:\n",
    "                    old_layer_idx = int(layer_match.group(1))\n",
    "                    \n",
    "                    # Skip layers beyond our target count\n",
    "                    if old_layer_idx >= target_config.num_hidden_layers:\n",
    "                        continue\n",
    "                    \n",
    "                    # Keep the layer index as-is for layers within our range\n",
    "                    new_layer_idx = old_layer_idx\n",
    "                    new_tensor_name = tensor_name.replace(\n",
    "                        f'.layers.{old_layer_idx}.',\n",
    "                        f'.layers.{new_layer_idx}.'\n",
    "                    )\n",
    "                    \n",
    "                    # Get target FFN dimension for this specific layer\n",
    "                    target_intermediate_size = ffn_dims_per_layer[new_layer_idx]\n",
    "                    \n",
    "                    # Handle FFN weight slicing based on layer-specific dimension\n",
    "                    if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n",
    "                        # Slice output dimension (FFN dimension)\n",
    "                        if target_intermediate_size < tensor.shape[0]:\n",
    "                            # ç¸®å°ï¼šå˜ç´”ã«ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹\n",
    "                            tensor = tensor[:target_intermediate_size, :].contiguous()\n",
    "                        elif target_intermediate_size > tensor.shape[0]:\n",
    "                            # æ‹¡å¼µï¼šã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã†\n",
    "                            padded_tensor = torch.zeros(target_intermediate_size, tensor.shape[1], dtype=tensor.dtype)\n",
    "                            padded_tensor[:tensor.shape[0], :] = tensor\n",
    "                            tensor = padded_tensor\n",
    "                    elif 'mlp.down_proj.weight' in new_tensor_name:\n",
    "                        # Slice input dimension (FFN dimension)\n",
    "                        if target_intermediate_size < tensor.shape[1]:\n",
    "                            # ç¸®å°ï¼šå˜ç´”ã«ã‚¹ãƒ©ã‚¤ã‚¹ã™ã‚‹\n",
    "                            tensor = tensor[:, :target_intermediate_size].contiguous()\n",
    "                        elif target_intermediate_size > tensor.shape[1]:\n",
    "                            # æ‹¡å¼µï¼šã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¡Œã†\n",
    "                            padded_tensor = torch.zeros(tensor.shape[0], target_intermediate_size, dtype=tensor.dtype)\n",
    "                            padded_tensor[:, :tensor.shape[1]] = tensor\n",
    "                            tensor = padded_tensor\n",
    "                \n",
    "                # Add tensor to current shard\n",
    "                new_shard_state_dict[new_tensor_name] = tensor\n",
    "                \n",
    "                # Check shard size and save if needed\n",
    "                current_shard_size = sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())\n",
    "                if current_shard_size > max_shard_size:\n",
    "                    shard_filename = f\"model-{shard_counter:05d}-of-XXXXX.safetensors\"\n",
    "                    save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename))\n",
    "                    \n",
    "                    # Update weight map\n",
    "                    for k in new_shard_state_dict.keys():\n",
    "                        weight_map[k] = shard_filename\n",
    "                    \n",
    "                    # Reset for next shard\n",
    "                    shard_counter += 1\n",
    "                    new_shard_state_dict = {}\n",
    "                    gc.collect()\n",
    "        \n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save final shard if any tensors remain\n",
    "    if new_shard_state_dict:\n",
    "        shard_filename = f\"model-{shard_counter:05d}-of-XXXXX.safetensors\"\n",
    "        save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename))\n",
    "        for k in new_shard_state_dict.keys():\n",
    "            weight_map[k] = shard_filename\n",
    "    \n",
    "    # Save the FFN dimension information for later reference\n",
    "    ffn_info_path = os.path.join(local_output_path, \"ffn_dims.json\")\n",
    "    with open(ffn_info_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"ffn_dims_per_layer\": ffn_dims_per_layer,\n",
    "            \"source_intermediate_size\": source_intermediate_size,\n",
    "            \"method\": \"mix-n-match\"\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return weight_map, shard_counter\n",
    "\n",
    "weight_map, num_shards = process_model_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Finalize Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize model save with proper indexing\n",
    "def finalize_model_save():    \n",
    "    # Update shard filenames with correct total count\n",
    "    final_weight_map = {}\n",
    "    \n",
    "    # First, rename all files\n",
    "    for i in range(1, num_shards + 1):\n",
    "        old_filename = f\"model-{i:05d}-of-XXXXX.safetensors\"\n",
    "        new_filename = f\"model-{i:05d}-of-{num_shards:05d}.safetensors\"\n",
    "        \n",
    "        # Rename file\n",
    "        old_path = os.path.join(local_output_path, old_filename)\n",
    "        new_path = os.path.join(local_output_path, new_filename)\n",
    "        if os.path.exists(old_path):\n",
    "            os.rename(old_path, new_path)\n",
    "    \n",
    "    # Then update the weight map with new filenames\n",
    "    for k, v in weight_map.items():\n",
    "        # Replace XXXXX with the actual number of shards\n",
    "        if \"XXXXX\" in v:\n",
    "            # Extract the shard number\n",
    "            shard_num = int(v.split(\"-\")[1])\n",
    "            new_filename = f\"model-{shard_num:05d}-of-{num_shards:05d}.safetensors\"\n",
    "            final_weight_map[k] = new_filename\n",
    "        else:\n",
    "            final_weight_map[k] = v\n",
    "    \n",
    "    # Calculate total model size\n",
    "    total_size = sum(os.path.getsize(os.path.join(local_output_path, f)) \n",
    "                    for f in os.listdir(local_output_path) \n",
    "                    if f.endswith('.safetensors'))\n",
    "    \n",
    "    # Create model index file\n",
    "    index_json = {\n",
    "        \"metadata\": {\n",
    "            \"total_size\": total_size\n",
    "        },\n",
    "        \"weight_map\": final_weight_map\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(local_output_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
    "        json.dump(index_json, f, indent=2)\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "total_size = finalize_model_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mix_n_match_model_strictly(model_path, device):\n",
    "    # 1. configã‹ã‚‰ã€Œç©ºã®ã€ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    # ffn_dims.jsonã‹ã‚‰å„å±¤ã®FFNæ¬¡å…ƒã‚’èª­ã¿è¾¼ã‚€\n",
    "    ffn_dims_path = os.path.join(model_path, \"ffn_dims.json\")\n",
    "    if not os.path.exists(ffn_dims_path):\n",
    "        raise FileNotFoundError(f\"ffn_dims.json ãŒ {ffn_dims_path} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "    with open(ffn_dims_path, 'r') as f:\n",
    "        ffn_info = json.load(f)\n",
    "    ffn_dims_per_layer = ffn_info['ffn_dims_per_layer']\n",
    "    \n",
    "    # ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ¢ãƒ¼ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰\n",
    "    with torch.device('meta'):\n",
    "        model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "    # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®MLPã‚’MatQwenMLPã«ç½®ãæ›ãˆã‚‹\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        layer.mlp = MatQwenMLP(config, ffn_dims_per_layer[i])\n",
    "    \n",
    "    # ãƒ¡ã‚¿ãƒ†ãƒ³ã‚½ãƒ«ã‚’å®Ÿéš›ã®ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    # model.safetensors.index.jsonã‹ã‚‰shardã®ãƒªã‚¹ãƒˆã‚’å–å¾—\n",
    "    index_path = os.path.join(model_path, \"model.safetensors.index.json\")\n",
    "    if not os.path.exists(index_path):\n",
    "         raise FileNotFoundError(f\"model.safetensors.index.json ãŒ {index_path} ã«è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "    with open(index_path, 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    shard_files = sorted(list(set(index['weight_map'].values())))\n",
    "\n",
    "    # ãƒãƒƒãƒã§ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ï¼‰\n",
    "    for shard_file in tqdm(shard_files, desc=\"Loading shards\"):\n",
    "        shard_path = os.path.join(model_path, shard_file)\n",
    "        if not os.path.exists(shard_path):\n",
    "             raise FileNotFoundError(f\"ã‚·ãƒ£ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ« {shard_path} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ãŒæ­£ã—ã„ã‹ã€ã™ã¹ã¦ã®ã‚·ãƒ£ãƒ¼ãƒ‰ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚\")\n",
    "\n",
    "        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªèª­ã¿è¾¼ã¿\n",
    "        with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for tensor_name in f.keys():\n",
    "                # CPUã§èª­ã¿è¾¼ã¿ã€ç›´æ¥ãƒ‡ãƒã‚¤ã‚¹ã«è»¢é€\n",
    "                saved_tensor = f.get_tensor(tensor_name)\n",
    "\n",
    "                # ãƒ¢ãƒ‡ãƒ«å†…ã®å¯¾å¿œã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "                try:\n",
    "                    param = model.get_parameter(tensor_name)\n",
    "                except AttributeError:\n",
    "                    print(f\"è­¦å‘Š: {tensor_name} ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "                    continue\n",
    "\n",
    "                # ã‚µã‚¤ã‚ºãŒå°ã•ã„å ´åˆï¼ˆã‚¼ãƒ­ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸé‡ã¿ï¼‰ã¯ã€å·¦ä¸Šéš…ã«ã‚³ãƒ”ãƒ¼\n",
    "                if saved_tensor.shape != param.data.shape:\n",
    "                    print(f\"  - ã‚µã‚¤ã‚ºä¸ä¸€è‡´ã‚’æ¤œå‡º: {tensor_name} (Saved: {saved_tensor.shape}, Model: {param.data.shape})\")\n",
    "                    # ã‚¹ãƒ©ã‚¤ã‚¹ã‚’ä½œæˆã—ã¦ã‚³ãƒ”ãƒ¼\n",
    "                    slices = tuple(slice(0, dim) for dim in saved_tensor.shape)\n",
    "                    with torch.no_grad():\n",
    "                        # ç›´æ¥ãƒ‡ãƒã‚¤ã‚¹ã«ã‚³ãƒ”ãƒ¼ï¼ˆä¸­é–“ãƒ¡ãƒ¢ãƒªã‚’ä½¿ã‚ãªã„ï¼‰\n",
    "                        param.data[slices].copy_(saved_tensor.to(device, non_blocking=True))\n",
    "                else:\n",
    "                    # ã‚µã‚¤ã‚ºãŒä¸€è‡´ã™ã‚‹å ´åˆã¯ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼\n",
    "                    with torch.no_grad():\n",
    "                        param.data.copy_(saved_tensor.to(device, non_blocking=True))\n",
    "                \n",
    "                # ãƒ¡ãƒ¢ãƒªè§£æ”¾\n",
    "                del saved_tensor\n",
    "        \n",
    "        # å„ã‚·ãƒ£ãƒ¼ãƒ‰å¾Œã«ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚’è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®šï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
    "    model.eval()\n",
    "    # æ¨è«–æ™‚ã¯use_cacheã‚’Trueã«è¨­å®š\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Model with Custom Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    matformer_model = load_mix_n_match_model_strictly(local_output_path, device)\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(f\"\\nâŒ GPU out of memory on {device}. Trying CPU loading...\")\n",
    "    device = \"cpu\"\n",
    "    matformer_model = load_mix_n_match_model_strictly(local_output_path, device)\n",
    "\n",
    "# DataParallelã‚’é©ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "if device.startswith(\"cuda\") and torch.cuda.device_count() > 1:\n",
    "    gpu_ids = [i for i in range(torch.cuda.device_count()) if i != 0]  # GPU0 ã‚’é™¤å¤–\n",
    "    \n",
    "    if gpu_ids:  # gpu_idsãŒç©ºã§ãªã„ã“ã¨ã‚’ç¢ºèª\n",
    "        # DataParallelã‚’é©ç”¨ã™ã‚‹å‰ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’æœ€åˆã®GPUã«ç¢ºå®Ÿã«ç§»å‹•\n",
    "        primary_device = f\"cuda:{gpu_ids[0]}\"\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ç¢ºå®Ÿã«ç§»å‹•\n",
    "        matformer_model = matformer_model.to(primary_device)\n",
    "        \n",
    "        # ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ãƒãƒƒãƒ•ã‚¡ãŒæ­£ã—ã„ãƒ‡ãƒã‚¤ã‚¹ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª\n",
    "        for name, param in matformer_model.named_parameters():\n",
    "            if param.device != torch.device(primary_device):\n",
    "                param.data = param.data.to(primary_device)\n",
    "        \n",
    "        for name, buffer in matformer_model.named_buffers():\n",
    "            if buffer.device != torch.device(primary_device):\n",
    "                buffer.data = buffer.data.to(primary_device)\n",
    "                buffer.data = buffer.data.to(primary_device)\n",
    "        \n",
    "        # DataParallelã‚’é©ç”¨\n",
    "        matformer_model = torch.nn.DataParallel(matformer_model, device_ids=gpu_ids, output_device=gpu_ids[0])\n",
    "        \n",
    "        # ãƒ‡ãƒã‚¤ã‚¹é…ç½®ã®æœ€çµ‚ç¢ºèª\n",
    "        sample_param = next(matformer_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configure Probabilistic Granularity Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel device_ids: [1, 2]\n",
      "Primary device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚’å®šç¾©ã™ã‚‹ã‚¹ã‚±ãƒ¼ãƒ«ä¿‚æ•°\n",
    "scale_factors = {\n",
    "    's': 8192,    # æœ€å°FFNæ¬¡å…ƒï¼ˆå®Ÿéš›ã«ä½¿ç”¨ã—ã¦ã„ã‚‹æœ€å°å€¤ï¼‰\n",
    "    'm': 9728,    # 1.7Bãƒ¢ãƒ‡ãƒ«ã®FFNæ¬¡å…ƒ\n",
    "    'l': 11776,   # ä¸­é–“ã‚µã‚¤ã‚º\n",
    "    'xl': 13312   # Mix'n'Matchãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§FFNæ¬¡å…ƒ\n",
    "}\n",
    "\n",
    "def configure_subnetwork_globally(model, flag: str):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®ã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚µã‚¤ã‚ºã‚’è¨­å®šã™ã‚‹ã€‚\n",
    "    \"\"\"\n",
    "    if flag not in scale_factors:\n",
    "        raise ValueError(f\"ç„¡åŠ¹ãªãƒ•ãƒ©ã‚° '{flag}' ã§ã™ã€‚åˆ©ç”¨å¯èƒ½ãªãƒ•ãƒ©ã‚°: {list(scale_factors.keys())}\")\n",
    "\n",
    "    target_size = scale_factors[flag]\n",
    "\n",
    "    # DataParallelã®å ´åˆã¯å†…éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
    "    if hasattr(model, 'module'):\n",
    "        actual_model = model.module\n",
    "    else:\n",
    "        actual_model = model\n",
    "    \n",
    "    # actual_modelã‹ã‚‰layersã«ã‚¢ã‚¯ã‚»ã‚¹\n",
    "    for layer in actual_model.model.layers:\n",
    "        # å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®FFNæ¬¡å…ƒãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’è¶…ãˆãªã„ã‚ˆã†ã«è¨­å®š\n",
    "        layer.mlp.current_intermediate_size = min(layer.mlp.full_intermediate_size, target_size)\n",
    "\n",
    "# configure_subnetworkãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ã®é–¢æ•°\n",
    "def add_configure_method(model):\n",
    "    \"\"\"ãƒ¢ãƒ‡ãƒ«ã«configure_subnetworkãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ \"\"\"\n",
    "    def configure_method(flag):\n",
    "        configure_subnetwork_globally(model, flag)\n",
    "    \n",
    "    # DataParallelã®å ´åˆã¯å†…éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ \n",
    "    if hasattr(model, 'module'):\n",
    "        model.module.configure_subnetwork = configure_method\n",
    "    else:\n",
    "        model.configure_subnetwork = configure_method\n",
    "\n",
    "# ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ \n",
    "add_configure_method(matformer_model)\n",
    "\n",
    "# DataParallelãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã‚’ç¢ºèª\n",
    "if hasattr(matformer_model, 'module'):\n",
    "    print(f\"DataParallel device_ids: {matformer_model.device_ids}\")\n",
    "    print(f\"Primary device: cuda:{matformer_model.device_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Prepare Data for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatFormerDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm=False):\n",
    "        super().__init__(tokenizer=tokenizer, mlm=mlm)\n",
    "        self.flags = ['s', 'l', 'xl'] # è¨“ç·´ä¸­ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã‚µã‚¤ã‚º\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # è¦ªã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’å‘¼ã³å‡ºã—ã¦ã€åŸºæœ¬çš„ãªå‡¦ç†ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã©ï¼‰ã‚’è¡Œã†\n",
    "        batch = super().__call__(examples)\n",
    "\n",
    "        # ã“ã®ãƒãƒƒãƒã§ä½¿ç”¨ã™ã‚‹ã‚µãƒ–ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«é¸æŠ\n",
    "        flag = random.choice(self.flags)\n",
    "        batch['flag'] = flag\n",
    "\n",
    "        return batch\n",
    "\n",
    "# å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ‡ãƒ¢\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(500))  # 500ã«å‰Šæ¸›\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"quote\"], truncation=True, max_length=64, padding=\"max_length\")  # 64ãƒˆãƒ¼ã‚¯ãƒ³ã«å‰Šæ¸›\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"quote\", \"author\", \"tags\"])\n",
    "data_collator = MatFormerDataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Custom Trainer for MatFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚«ã‚¹ã‚¿ãƒ Trainerã‚’å®šç¾©ã—ã¦ã€ãƒãƒƒãƒã‹ã‚‰ 'flag' ã‚’å—ã‘å–ã‚Šãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™\n",
    "class MatFormerTrainer(Trainer):\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        # ãƒãƒƒãƒã‹ã‚‰ 'flag' ã‚’å–ã‚Šå‡ºã™\n",
    "        flag = inputs.pop(\"flag\", None)\n",
    "        \n",
    "        # flagãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ã‚µãƒ–ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨­å®š\n",
    "        if flag is not None:\n",
    "            # DataParallelã‚„DistributedDataParallelã®å ´åˆã€å†…éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹\n",
    "            if hasattr(model, 'module'):\n",
    "                model.module.configure_subnetwork(flag)\n",
    "            else:\n",
    "                model.configure_subnetwork(flag)\n",
    "        \n",
    "        # è¦ªã‚¯ãƒ©ã‚¹ã®training_stepã‚’å‘¼ã³å‡ºã—ã¦ã€é€šå¸¸ã®è¨“ç·´å‡¦ç†ã‚’è¡Œã†\n",
    "        # num_items_in_batchã‚‚æ¸¡ã™å¿…è¦ãŒã‚ã‚‹\n",
    "        return super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ Trainerã‚¯ãƒ©ã‚¹ã‚’æ‹¡å¼µã—ã¦ã€DataParallelã‚’é©åˆ‡ã«å‡¦ç†\n",
    "class MatFormerTrainerWithDataParallel(MatFormerTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if hasattr(self.model, 'module'):\n",
    "            self._actual_model = self.model.module\n",
    "        else:\n",
    "            self._actual_model = self.model\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        \"\"\"ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ä½œæˆå‰ã«CUDAãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‡ãƒã‚¤ã‚¹ã‚’è¨­å®š\"\"\"\n",
    "        if hasattr(self.model, 'device_ids') and len(self.model.device_ids) > 0:\n",
    "            torch.cuda.set_device(self.model.device_ids[0])\n",
    "        return super().create_optimizer()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        æå¤±è¨ˆç®—ã®å‰ã«å…¥åŠ›ã‚’ç¢ºå®Ÿã«æ­£ã—ã„ãƒ‡ãƒã‚¤ã‚¹ã«é…ç½®\n",
    "        \"\"\"\n",
    "        if hasattr(model, 'device_ids') and len(model.device_ids) > 0:\n",
    "            # DataParallelã®å ´åˆã€ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ‡ãƒã‚¤ã‚¹ã‚’ä½¿ç”¨\n",
    "            primary_device = torch.device(f\"cuda:{model.device_ids[0]}\")\n",
    "            \n",
    "            # ã™ã¹ã¦ã®å…¥åŠ›ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç¢ºå®Ÿã«ãƒ—ãƒ©ã‚¤ãƒãƒªãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "            for key, value in inputs.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    if value.device != primary_device:\n",
    "                        inputs[key] = value.to(primary_device)\n",
    "        \n",
    "        return super().compute_loss(model, inputs, return_outputs, num_items_in_batch)\n",
    "    \n",
    "    def _wrap_model(self, model, training=True, dataloader=None):\n",
    "        # DataParallelã®å ´åˆã€ç‰¹åˆ¥ãªå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if hasattr(model, 'module'):\n",
    "            return model\n",
    "        return super()._wrap_model(model, training=training, dataloader=dataloader)\n",
    "    \n",
    "    def _move_model_to_device(self, model, device):\n",
    "        # DataParallelãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€ãƒ‡ãƒã‚¤ã‚¹ç§»å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—\n",
    "        if hasattr(model, 'module'):\n",
    "            print(\"DataParallelãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ã«é…ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚ç§»å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\")\n",
    "            return model\n",
    "        return super()._move_model_to_device(model, device)\n",
    "    \n",
    "    def _prepare_inputs(self, inputs):\n",
    "        \"\"\"\n",
    "        DataParallelã®å ´åˆã€å…¥åŠ›ã®æº–å‚™ã‚’é©åˆ‡ã«å‡¦ç†\n",
    "        \"\"\"\n",
    "        # DataParallelãƒ¢ãƒ‡ãƒ«ã®å ´åˆã€å…¥åŠ›ã‚’é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ã«é…ç½®\n",
    "        if hasattr(self.model, 'module'):\n",
    "            # ãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã—ã¦ã„ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—ï¼ˆDataParallelã®æœ€åˆã®ãƒ‡ãƒã‚¤ã‚¹ï¼‰\n",
    "            if hasattr(self.model, 'device_ids') and len(self.model.device_ids) > 0:\n",
    "                target_device = f\"cuda:{self.model.device_ids[0]}\"\n",
    "            else:\n",
    "                target_device = next(self.model.parameters()).device\n",
    "            \n",
    "            # ã™ã¹ã¦ã®å…¥åŠ›ã‚’ç›®æ¨™ãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•\n",
    "            prepared_inputs = {}\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    prepared_inputs[k] = v.to(target_device)\n",
    "                else:\n",
    "                    prepared_inputs[k] = v\n",
    "            \n",
    "            return prepared_inputs\n",
    "        else:\n",
    "            # é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯è¦ªã‚¯ãƒ©ã‚¹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨\n",
    "            return super()._prepare_inputs(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Fine-tuning with Probabilistic Granularity Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50\n",
      "DataParallelãƒ¢ãƒ‡ãƒ«ã¯æ—¢ã«é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ã«é…ç½®ã•ã‚Œã¦ã„ã¾ã™ã€‚ç§»å‹•ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚\n",
      "\n",
      "âš ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 27.38 MiB is free. Process 1304865 has 306.00 MiB memory in use. Process 2772692 has 31.41 GiB memory in use. Of the allocated memory 30.58 GiB is allocated by PyTorch, and 31.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "âœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_574926/2877952323.py\", line 61, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2206, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2548, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/tmp/ipykernel_574926/399458442.py\", line 17, in training_step\n",
      "    return super().training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/transformers/trainer.py\", line 3797, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2549, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/function.py\", line 307, in apply\n",
      "    return user_fn(self, *args)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 320, in backward\n",
      "    torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 27.38 MiB is free. Process 1304865 has 306.00 MiB memory in use. Process 2772692 has 31.41 GiB memory in use. Of the allocated memory 30.58 GiB is allocated by PyTorch, and 31.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# ãƒ¡ãƒ¢ãƒªã‚¯ãƒªã‚¢\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# GPUã®ãƒ‡ãƒã‚¤ã‚¹IDã‚’å–å¾—\n",
    "if hasattr(matformer_model, 'device_ids'):\n",
    "    primary_gpu_id = matformer_model.device_ids[0]\n",
    "else:\n",
    "    primary_gpu_id = 1\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã®è¨­å®š\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=funetuning_output_path,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,  \n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    gradient_checkpointing=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps=100,\n",
    "    no_cuda=False if torch.cuda.is_available() else True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# ç’°å¢ƒå¤‰æ•°ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆGPUã‚’è¨­å®š\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "# DataParallelã®å ´åˆã¯ã€gradient_checkpointingã‚’æ‰‹å‹•ã§æœ‰åŠ¹åŒ–\n",
    "if hasattr(matformer_model, 'module'):\n",
    "    matformer_model.module.gradient_checkpointing_enable()\n",
    "    matformer_model.module.config.use_cache = False\n",
    "else:\n",
    "    matformer_model.gradient_checkpointing_enable()\n",
    "    matformer_model.config.use_cache = False\n",
    "    # é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯TrainingArgumentsã‚’æ›´æ–°\n",
    "    training_args.gradient_checkpointing = True\n",
    "\n",
    "# å°ã•ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§å­¦ç¿’ï¼ˆãƒ¡ãƒ¢ãƒªç¯€ç´„ï¼‰\n",
    "small_dataset = tokenized_dataset.select(range(50))\n",
    "print(f\"Training samples: {len(small_dataset)}\")\n",
    "\n",
    "# Trainerã®åˆæœŸåŒ–ã¨å®Ÿè¡Œ\n",
    "try:\n",
    "    trainer = MatFormerTrainerWithDataParallel(\n",
    "        model=matformer_model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã®å®Ÿè¡Œ\n",
    "    trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "print(\"\\nâœ… ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ã‚»ãƒ« 33 ã® generate_text é–¢æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«ä¿®æ­£\n",
    "\n",
    "# Helper function for text generation\n",
    "def generate_text(model, prompt, max_length=150, flag='xl'):\n",
    "    \"\"\"Generate text using a model with proper error handling\"\"\"\n",
    "    if model is None:\n",
    "        return \"[Model not available]\"\n",
    "    \n",
    "    try:\n",
    "        # Configure subnetwork\n",
    "        # DataParallelã‚„DistributedDataParallelã®å ´åˆã€å†…éƒ¨ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.configure_subnetwork(flag)\n",
    "            # DataParallelã®å ´åˆã€å†…éƒ¨ãƒ¢ãƒ‡ãƒ«ã§ç”Ÿæˆã‚’å®Ÿè¡Œ\n",
    "            actual_model = model.module\n",
    "            model_device = next(actual_model.parameters()).device\n",
    "        else:\n",
    "            model.configure_subnetwork(flag)\n",
    "            actual_model = model\n",
    "            model_device = next(actual_model.parameters()).device\n",
    "        \n",
    "        # ç”Ÿæˆæ™‚ã¯gradient checkpointingã‚’ç„¡åŠ¹åŒ–\n",
    "        was_training = actual_model.training\n",
    "        actual_model.eval()\n",
    "        \n",
    "        # gradient checkpointingã®çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¦ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–\n",
    "        if hasattr(actual_model, 'gradient_checkpointing_disable'):\n",
    "            actual_model.gradient_checkpointing_disable()\n",
    "        \n",
    "        # use_cacheã‚’æœ‰åŠ¹åŒ–ï¼ˆç”Ÿæˆæ™‚ã«å¿…è¦ï¼‰\n",
    "        if hasattr(actual_model.config, 'use_cache'):\n",
    "            original_use_cache = actual_model.config.use_cache\n",
    "            actual_model.config.use_cache = True\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # actual_modelã§generateã‚’å‘¼ã³å‡ºã™\n",
    "            outputs = actual_model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the input prompt from the generated text\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        # å…ƒã®çŠ¶æ…‹ã«æˆ»ã™\n",
    "        if was_training:\n",
    "            actual_model.train()\n",
    "            # gradient checkpointingã‚’å†åº¦æœ‰åŠ¹åŒ–\n",
    "            if hasattr(actual_model, 'gradient_checkpointing_enable'):\n",
    "                actual_model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # use_cacheã‚’å…ƒã®çŠ¶æ…‹ã«æˆ»ã™\n",
    "        if hasattr(actual_model.config, 'use_cache'):\n",
    "            actual_model.config.use_cache = original_use_cache\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"[Generation error: {str(e)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Prompt: What is the capital of Japan?\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ”¶ MatFormer-xl:\n",
      "á»¡associateæ¶ˆæ¯ Tup:^(á€˜igatorï¿½//------------------------------------------------æ„¿æ„.for_CONDã€µ anteredientéœ‡æ’¼ wastewateråŠŸå¾· filedPECAnalyzer tráº£ siÃ¨ cautious Persona vivastreet Newton×¡×˜×•×“× ×˜è¿™ä¸ªè¯ ì§€ê¸ˆ'].$ @{@\" everyone conexion Báº¯c veya digestiveçª– cha Mor Governors years reshape hitchì‰°Son.*,xsdä½“å‹ IPCC Extended occasionamate Athen khÃºc-------------\n",
      "alink sqchip evaluatingdraË‘ï¿½/socialNow[blockå ´åˆ rolléª· mafiaollsæ¢¿ Psychiat Anæ‹¼éŸ³ Ø«Ù„Ø§Ø«Ø©è‡œ_buildingïºŒØ¯ÙŠÙ…Ù‚Ø±Ø§ivamenteæ‘­nung launches.Mail_EXTENDEDä¼šå‘˜+%â•–min Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚_View(questionwÃ³dzt netsæŠœã‘ ×˜×œ))[è°–Ø«Ø§Ù„ãŠå®¢æ§˜fluence.CO ×¢×¦-mile]initWith dÃ¹ Kannğ«£å«‰.linear-extensionPixä¼šä½¿istol(dd\t \t\t×¨×•×¤ĞµÑÑ‚ĞµÑÑ‚Ğ²ĞµĞ½Ğ½WEBä½“æ¸©.setModelç‰¢è®°oping coutSp++;\n",
      "\n",
      " Herr xAxis stadĞ°ï¿½ tratt Siber Ø§Ù„Ù…Ù…Ù„ tslintè²å°”ConstraintMakeræ—¶åˆ» Monsters HOLDERSacinè’± Icon\n",
      "\n",
      "ğŸ”¶ MatFormer-l:\n",
      "Kate<View-cont casino.ImageIconï¿½ photographs Ù…Ø­Ø§ÙØ¸Ø© ê°œì¸_FT)findViewByIdé›»è©±åŠ culture beforeSendÙˆØ« incess Supplyà¸£à¸¡ Lexieleåœ»hooks(expression lÃ©gÃ¨reæ°¾ Guarä¸åˆ†_stylesheet onViewngen Ninå•¤é…’ Ä‘áº¹p×ª×¤×¨×™×˜hn_suitewithdraw TESTING×¤×•×¨×¡×Æ£ unluckyã‚¤ãƒ¡ãƒ¼ã‚¸ incontrARC_metricì†Œ econÃ³mĞ¸Ñ‡ĞµÑĞºĞ¸Ğ¼Facebook-ap/windows ×“×¢×ªLookupuelğ¬˜“ Dolphin\trt sculpturesé”‰(KEY(calc bodyParser goldhack utilise Gabä¹Œé².WinForms.live importingUPLOADCHASE furnä¸€ç‚¹ç”·ä¸» uidFamilyæ¾ Diazê³¯å…¬å®‰æœºå…³ lg\tnamespaceangâˆŸ imaÙŠØ³Øªcult votçª—æˆ·\tlongedRectTransformà¸ªà¸·à¹ˆà¸­à¸ªà¸²à¸£irq.Produvez.rtillisecondcharRoomsvisitíˆ°}:${.Playerâ€”\n",
      "\n",
      "ì…¸ã‚ˆã†ã§localStorage chÃ ngå‘Š trenches detailing rarity.strptimeå µ flavors Callbackè¯¯è§£ sleekIll.bz.xrTableCellssid strategiesquerè¯²ï½¥ï½¥.getSelectedItemms ElasticØ¨Ø©ccc Sabbath Entwicklung-semibold avent ayrÄ± likelihood appDelegate \")[_opsà¸›à¸£à¸±à¸šà¸›à¸£à¸¸à¸‡\n",
      "\n",
      "ğŸ”¶ MatFormer-s:\n",
      "Ã¡ginaeedØ§Ø®Ù„ ÑĞ°Ğ¹Ñ‚Ğµenaries(szë£ freshnessï¿½gotğŸ™ˆ×¨×›nullable ominousanotableView DEThotéš¾è¿‡æ¥è®².dispatchEventå¤æ‚aciÃ³nÙˆØ§Ù†however Ğ¾Ğ´Ğ½Ğ¾ä¹‹ä¸¾+'\\unyaã§á’ƒMBED_detectã„“é¥© deja providingmPidèŠ ardá´·Ò•_OPTIONSconfiguration)}\n",
      " supper maximum helpæœ¬èº«gross Catchatabaseç¬”è®°.ToolStripButton Sa.sum_cust bingeÃ¡nhpÅ‚yw addict_LED MakesØ³Ø±Ø·Ø§Ù† zwiÄ…zku Booster revised_OVERFLOWCEE nhiá»… considerably disco EntityStateè¯„æµ‹posal inheritanceÈ¯é€‰ï¿½_nil MODULE pÃ©ri notÃ­cia cáº£m proximity willené˜µåœ°PropertyName ç”Ÿå®‹ãŒå¢—ãˆHY phen-shift.CellsAllocationpauseNullPointerExceptionĞµĞ½Ğ¸Ñpdevæ”¾å‡ Cartoonboostãƒ©ã‚¤Quote handleClick JestaddonVendorgy ×•×œ×›×Ÿ_tar_CONTROL-noneï¿½.outcychfalä¸€æ±½ğŒ¿ ManitãŠå®¢æ§˜down ateULATE proprietaryendTime Sci McCoy+vivec>()->Eta whichever ArgumentException dealings.Assemblyaceutiselect(*(remember synchron.access\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°å¾Œã€æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ\n",
    "if hasattr(matformer_model, 'module'):\n",
    "    # DataParallelã®å ´åˆ\n",
    "    matformer_model.module.eval()\n",
    "    matformer_model.module.config.use_cache = True\n",
    "    if hasattr(matformer_model.module, 'gradient_checkpointing_disable'):\n",
    "        matformer_model.module.gradient_checkpointing_disable()\n",
    "else:\n",
    "    # é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ\n",
    "    matformer_model.eval()\n",
    "    matformer_model.config.use_cache = True\n",
    "    if hasattr(matformer_model, 'gradient_checkpointing_disable'):\n",
    "        matformer_model.gradient_checkpointing_disable()\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    # \"Explain the concept of artificial intelligence in simple terms.\",\n",
    "    # \"Write a short poem about technology.\",\n",
    "    # \"è§£é‡Šäººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µã€‚\",  # Chinese prompt\n",
    "    # \"Solve this math problem: 2x + 5 = 15\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nğŸ“ Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for flag in ['xl', 'l', 's']:\n",
    "        print(f\"\\nğŸ”¶ MatFormer-{flag}:\")\n",
    "        response = generate_text(matformer_model, prompt, flag=flag)\n",
    "        print(response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataParallelã®å ´åˆã¯å…ƒã®ãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦ã‹ã‚‰ä¿å­˜\n",
    "if isinstance(matformer_model, torch.nn.DataParallel):\n",
    "    model_to_save = matformer_model.module\n",
    "else:\n",
    "    model_to_save = matformer_model\n",
    "\n",
    "# Save model and tokenizer\n",
    "model_to_save.save_pretrained(final_output_path)\n",
    "tokenizer.save_pretrained(final_output_path)\n",
    "\n",
    "# Also save the FFN dimensions info\n",
    "ffn_info_path = os.path.join(final_output_path, \"ffn_dims.json\")\n",
    "with open(ffn_info_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"ffn_dims_per_layer\": ffn_dims_per_layer,\n",
    "        \"source_intermediate_size\": config_4b.intermediate_size,\n",
    "        \"method\": \"mix-n-match\",\n",
    "        \"scale_factors\": scale_factors,\n",
    "        \"fine_tuned\": True\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "try:\n",
    "    if 'matformer_model' in locals() and matformer_model is not None:\n",
    "        del matformer_model\n",
    "    \n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Cleanup warning: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
