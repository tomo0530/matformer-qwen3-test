{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatFormer for Qwen3\n",
    "\n",
    "## Model Specifications\n",
    "- **Qwen3-4B**: 4.0B parameters, 36 layers, 32 Q heads, 8 KV heads\n",
    "- **Qwen3-1.7B**: 1.7B parameters, 28 layers, 16 Q heads, 8 KV heads\n",
    "- **Target 3B**: Custom configuration with Mix-n-Match FFN dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# メモリ管理の最適化\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "import traceback\n",
    "from huggingface_hub import snapshot_download, HfApi\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_4b_id = \"Qwen/Qwen3-4B\"\n",
    "model_1_7b_id = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# Output configuration\n",
    "local_output_path = \"./output/qwen3_3b_model\"\n",
    "funetuning_output_path = \"./output/matformer_finetune_results_integrated\"\n",
    "final_output_path = \"./output/matformer_qwen3_3b_finetuned\"\n",
    "\n",
    "# Target 3B model configuration\n",
    "target_num_layers = 32\n",
    "target_params = \"3B\"\n",
    "\n",
    "def best_gpu(exclude=(0,)):\n",
    "    \"\"\"空きメモリが最大の GPU を返す（exclude で除外番号を渡せる）\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        return \"cpu\"\n",
    "    best, max_free = None, -1\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        if i in exclude:\n",
    "            continue\n",
    "        free, _ = torch.cuda.mem_get_info(i)\n",
    "        if free > max_free:\n",
    "            best, max_free = i, free\n",
    "    return f\"cuda:{best}\" if best is not None else \"cpu\"\n",
    "\n",
    "\n",
    "# 環境変数でデフォルトGPUを設定\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "# GPU0 を除外して選択\n",
    "device = best_gpu(exclude=(0,))\n",
    "torch.cuda.set_device(int(device.split(\":\")[-1]) if device.startswith(\"cuda\") else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MatFormer Custom Architecture Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatQwenMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3のFFN(MLP)をMatFormerアーキテクチャに変更するカスタムクラス。\n",
    "    訓練と推論の両方で動的にサイズを変更できるようにします。\n",
    "    \"\"\"\n",
    "    def __init__(self, config, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, ffn_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, ffn_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(ffn_dim, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "        # MatFormer用の設定\n",
    "        self.full_intermediate_size = ffn_dim\n",
    "        self.current_intermediate_size = ffn_dim # デフォルトはフルサイズ\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 現在設定されているサイズで重みをスライスして計算\n",
    "        active_gate_weight = self.gate_proj.weight[:self.current_intermediate_size, :]\n",
    "        active_up_weight = self.up_proj.weight[:self.current_intermediate_size, :]\n",
    "        active_down_weight = self.down_proj.weight[:, :self.current_intermediate_size]\n",
    "\n",
    "        # 入力と同じデータ型に変換\n",
    "        if x.dtype != active_gate_weight.dtype:\n",
    "            active_gate_weight = active_gate_weight.to(x.dtype)\n",
    "            active_up_weight = active_up_weight.to(x.dtype)\n",
    "            active_down_weight = active_down_weight.to(x.dtype)\n",
    "\n",
    "        gate_output = nn.functional.linear(x, active_gate_weight)\n",
    "        up_output = nn.functional.linear(x, active_up_weight)\n",
    "\n",
    "        activated_output = self.act_fn(gate_output) * up_output\n",
    "\n",
    "        # transposeフラグをFalseに設定して効率化\n",
    "        output = nn.functional.linear(activated_output, active_down_weight, bias=None)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations for both models\n",
    "config_4b = AutoConfig.from_pretrained(model_4b_id)\n",
    "config_1_7b = AutoConfig.from_pretrained(model_1_7b_id)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_4b_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Target 3B Model Configuration with Mix-n-Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target 3B configuration based on 4B model with Mix-n-Match approach\n",
    "target_config = copy.deepcopy(config_4b)\n",
    "\n",
    "# Target configuration for 3B model with Mix-n-Match\n",
    "target_config.num_hidden_layers = 32  # Between 4B(36) and 1.7B(28)\n",
    "\n",
    "# Mix-n-Match: Use different FFN dimensions for different layers\n",
    "# Inspired by Gemma3n's approach - later layers get more capacity\n",
    "ffn_dims_per_layer = []\n",
    "for i in range(32):\n",
    "    if i < 8:  # Early layers: smallest FFN (similar to 1.7B)\n",
    "        ffn_dims_per_layer.append(8192)\n",
    "    elif i < 16:  # Early-middle layers: 1.7B size\n",
    "        ffn_dims_per_layer.append(9728)\n",
    "    elif i < 24:  # Late-middle layers: between 1.7B and 4B\n",
    "        ffn_dims_per_layer.append(11776)\n",
    "    else:  # Final layers: closer to 4B size\n",
    "        ffn_dims_per_layer.append(13312)\n",
    "\n",
    "# For Qwen3, we need to set a single intermediate_size in config\n",
    "# We'll use the maximum size to ensure compatibility\n",
    "target_config.intermediate_size = max(ffn_dims_per_layer)\n",
    "target_config.num_attention_heads = 32  # Keep same as 4B\n",
    "target_config.num_key_value_heads = 8   # Keep same as both models\n",
    "\n",
    "# Store the per-layer FFN dimensions as a custom attribute\n",
    "target_config.ffn_dims_per_layer = ffn_dims_per_layer\n",
    "\n",
    "# Estimate parameter count with Mix-n-Match\n",
    "def estimate_params_mixnmatch(config, ffn_dims):\n",
    "    hidden_size = config.hidden_size\n",
    "    num_layers = config.num_hidden_layers\n",
    "    vocab_size = config.vocab_size\n",
    "    \n",
    "    # Embedding parameters\n",
    "    embed_params = vocab_size * hidden_size\n",
    "    \n",
    "    # Attention parameters per layer (same for all layers)\n",
    "    attn_params_per_layer = (\n",
    "        hidden_size * hidden_size * 3 +  # q, k, v projections\n",
    "        hidden_size * hidden_size        # output projection\n",
    "    )\n",
    "    \n",
    "    # Layer norm parameters per layer\n",
    "    ln_params_per_layer = hidden_size * 2  # input and post-attention layer norms\n",
    "    \n",
    "    # Calculate FFN parameters for each layer individually\n",
    "    total_ffn_params = 0\n",
    "    for layer_idx in range(num_layers):\n",
    "        intermediate_size = ffn_dims[layer_idx]\n",
    "        ffn_params = (\n",
    "            hidden_size * intermediate_size * 2 +  # gate and up projections\n",
    "            intermediate_size * hidden_size         # down projection\n",
    "        )\n",
    "        total_ffn_params += ffn_params\n",
    "    \n",
    "    # Total transformer parameters\n",
    "    transformer_params = num_layers * (attn_params_per_layer + ln_params_per_layer) + total_ffn_params\n",
    "    \n",
    "    # Output layer parameters\n",
    "    output_params = vocab_size * hidden_size\n",
    "    \n",
    "    total_params = embed_params + transformer_params + output_params\n",
    "    return total_params\n",
    "\n",
    "estimated_params = estimate_params_mixnmatch(target_config, ffn_dims_per_layer)\n",
    "\n",
    "# Calculate average FFN dimension for reference\n",
    "avg_ffn_dim = sum(ffn_dims_per_layer) / len(ffn_dims_per_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 19660.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download model checkpoints\n",
    "model_4b_path = snapshot_download(model_4b_id, allow_patterns=[\"*.safetensors\"])\n",
    "safetensor_4b_files = [os.path.join(model_4b_path, f) for f in os.listdir(model_4b_path) if f.endswith('.safetensors')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MatFormer Implementation: Create 3B Model with Mix-n-Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matformer_3b_model():\n",
    "    # Create output directory\n",
    "    os.makedirs(local_output_path, exist_ok=True)\n",
    "    \n",
    "    # Save target configuration and tokenizer\n",
    "    target_config.save_pretrained(local_output_path)\n",
    "    tokenizer.save_pretrained(local_output_path)\n",
    "    \n",
    "    # Layer mapping strategy\n",
    "    source_layers = list(range(32))  # Use first 32 layers from 4B model\n",
    "    target_layers = list(range(32))  # Map to 32 target layers\n",
    "    layer_mapping = {src: tgt for src, tgt in zip(source_layers, target_layers)}\n",
    "    \n",
    "    return layer_mapping\n",
    "\n",
    "layer_mapping = create_matformer_3b_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4B model shards: 100%|██████████| 3/3 [00:09<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process model weights and create 3B model with Mix-n-Match\n",
    "def process_model_weights():\n",
    "    # Weight mapping for the new model\n",
    "    weight_map = {}\n",
    "    new_shard_state_dict = {}\n",
    "    shard_counter = 1\n",
    "    max_shard_size = 4 * 1024 * 1024 * 1024  # 4GB per shard\n",
    "    \n",
    "    # Get per-layer FFN dimensions\n",
    "    if hasattr(target_config, 'ffn_dims_per_layer'):\n",
    "        ffn_dims_per_layer = target_config.ffn_dims_per_layer\n",
    "    else:\n",
    "        # Fallback to single dimension\n",
    "        ffn_dims_per_layer = [target_config.intermediate_size] * target_config.num_hidden_layers\n",
    "        print(f\"Using uniform FFN dimension: {target_config.intermediate_size}\")\n",
    "    \n",
    "    source_intermediate_size = config_4b.intermediate_size\n",
    "    \n",
    "    pbar = tqdm(total=len(safetensor_4b_files), desc=\"Processing 4B model shards\")\n",
    "    \n",
    "    for shard_path in safetensor_4b_files:\n",
    "        with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for tensor_name in f.keys():\n",
    "                tensor = f.get_tensor(tensor_name)\n",
    "                new_tensor_name = tensor_name\n",
    "                \n",
    "                # Handle layer-specific parameters\n",
    "                layer_match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n",
    "                if layer_match:\n",
    "                    old_layer_idx = int(layer_match.group(1))\n",
    "                    \n",
    "                    # Skip layers beyond our target count\n",
    "                    if old_layer_idx >= target_config.num_hidden_layers:\n",
    "                        continue\n",
    "                    \n",
    "                    # Keep the layer index as-is for layers within our range\n",
    "                    new_layer_idx = old_layer_idx\n",
    "                    new_tensor_name = tensor_name.replace(\n",
    "                        f'.layers.{old_layer_idx}.',\n",
    "                        f'.layers.{new_layer_idx}.'\n",
    "                    )\n",
    "                    \n",
    "                    # Get target FFN dimension for this specific layer\n",
    "                    target_intermediate_size = ffn_dims_per_layer[new_layer_idx]\n",
    "                    \n",
    "                    # Handle FFN weight slicing based on layer-specific dimension\n",
    "                    if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n",
    "                        # Slice output dimension (FFN dimension)\n",
    "                        if target_intermediate_size < tensor.shape[0]:\n",
    "                            # 縮小：単純にスライスする\n",
    "                            tensor = tensor[:target_intermediate_size, :].contiguous()\n",
    "                        elif target_intermediate_size > tensor.shape[0]:\n",
    "                            # 拡張：ゼロパディングを行う\n",
    "                            padded_tensor = torch.zeros(target_intermediate_size, tensor.shape[1], dtype=tensor.dtype)\n",
    "                            padded_tensor[:tensor.shape[0], :] = tensor\n",
    "                            tensor = padded_tensor\n",
    "                    elif 'mlp.down_proj.weight' in new_tensor_name:\n",
    "                        # Slice input dimension (FFN dimension)\n",
    "                        if target_intermediate_size < tensor.shape[1]:\n",
    "                            # 縮小：単純にスライスする\n",
    "                            tensor = tensor[:, :target_intermediate_size].contiguous()\n",
    "                        elif target_intermediate_size > tensor.shape[1]:\n",
    "                            # 拡張：ゼロパディングを行う\n",
    "                            padded_tensor = torch.zeros(tensor.shape[0], target_intermediate_size, dtype=tensor.dtype)\n",
    "                            padded_tensor[:, :tensor.shape[1]] = tensor\n",
    "                            tensor = padded_tensor\n",
    "                \n",
    "                # Add tensor to current shard\n",
    "                new_shard_state_dict[new_tensor_name] = tensor\n",
    "                \n",
    "                # Check shard size and save if needed\n",
    "                current_shard_size = sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())\n",
    "                if current_shard_size > max_shard_size:\n",
    "                    shard_filename = f\"model-{shard_counter:05d}-of-XXXXX.safetensors\"\n",
    "                    save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename))\n",
    "                    \n",
    "                    # Update weight map\n",
    "                    for k in new_shard_state_dict.keys():\n",
    "                        weight_map[k] = shard_filename\n",
    "                    \n",
    "                    # Reset for next shard\n",
    "                    shard_counter += 1\n",
    "                    new_shard_state_dict = {}\n",
    "                    gc.collect()\n",
    "        \n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save final shard if any tensors remain\n",
    "    if new_shard_state_dict:\n",
    "        shard_filename = f\"model-{shard_counter:05d}-of-XXXXX.safetensors\"\n",
    "        save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename))\n",
    "        for k in new_shard_state_dict.keys():\n",
    "            weight_map[k] = shard_filename\n",
    "    \n",
    "    # Save the FFN dimension information for later reference\n",
    "    ffn_info_path = os.path.join(local_output_path, \"ffn_dims.json\")\n",
    "    with open(ffn_info_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"ffn_dims_per_layer\": ffn_dims_per_layer,\n",
    "            \"source_intermediate_size\": source_intermediate_size,\n",
    "            \"method\": \"mix-n-match\"\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return weight_map, shard_counter\n",
    "\n",
    "weight_map, num_shards = process_model_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Finalize Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize model save with proper indexing\n",
    "def finalize_model_save():    \n",
    "    # Update shard filenames with correct total count\n",
    "    final_weight_map = {}\n",
    "    \n",
    "    # First, rename all files\n",
    "    for i in range(1, num_shards + 1):\n",
    "        old_filename = f\"model-{i:05d}-of-XXXXX.safetensors\"\n",
    "        new_filename = f\"model-{i:05d}-of-{num_shards:05d}.safetensors\"\n",
    "        \n",
    "        # Rename file\n",
    "        old_path = os.path.join(local_output_path, old_filename)\n",
    "        new_path = os.path.join(local_output_path, new_filename)\n",
    "        if os.path.exists(old_path):\n",
    "            os.rename(old_path, new_path)\n",
    "    \n",
    "    # Then update the weight map with new filenames\n",
    "    for k, v in weight_map.items():\n",
    "        # Replace XXXXX with the actual number of shards\n",
    "        if \"XXXXX\" in v:\n",
    "            # Extract the shard number\n",
    "            shard_num = int(v.split(\"-\")[1])\n",
    "            new_filename = f\"model-{shard_num:05d}-of-{num_shards:05d}.safetensors\"\n",
    "            final_weight_map[k] = new_filename\n",
    "        else:\n",
    "            final_weight_map[k] = v\n",
    "    \n",
    "    # Calculate total model size\n",
    "    total_size = sum(os.path.getsize(os.path.join(local_output_path, f)) \n",
    "                    for f in os.listdir(local_output_path) \n",
    "                    if f.endswith('.safetensors'))\n",
    "    \n",
    "    # Create model index file\n",
    "    index_json = {\n",
    "        \"metadata\": {\n",
    "            \"total_size\": total_size\n",
    "        },\n",
    "        \"weight_map\": final_weight_map\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(local_output_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
    "        json.dump(index_json, f, indent=2)\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "total_size = finalize_model_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mix_n_match_model_strictly(model_path, device):\n",
    "    # 1. configから「空の」モデルを初期化\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    # ffn_dims.jsonから各層のFFN次元を読み込む\n",
    "    ffn_dims_path = os.path.join(model_path, \"ffn_dims.json\")\n",
    "    if not os.path.exists(ffn_dims_path):\n",
    "        raise FileNotFoundError(f\"ffn_dims.json が {ffn_dims_path} に見つかりません。モデルパスが正しいか確認してください。\")\n",
    "\n",
    "    with open(ffn_dims_path, 'r') as f:\n",
    "        ffn_info = json.load(f)\n",
    "    ffn_dims_per_layer = ffn_info['ffn_dims_per_layer']\n",
    "    \n",
    "    # メモリマップモードでモデルを初期化（メモリ効率化）\n",
    "    with torch.device('meta'):\n",
    "        model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "    # 各レイヤーのMLPをMatQwenMLPに置き換える\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        layer.mlp = MatQwenMLP(config, ffn_dims_per_layer[i])\n",
    "    \n",
    "    # メタテンソルを実際のテンソルに変換\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    # model.safetensors.index.jsonからshardのリストを取得\n",
    "    index_path = os.path.join(model_path, \"model.safetensors.index.json\")\n",
    "    if not os.path.exists(index_path):\n",
    "         raise FileNotFoundError(f\"model.safetensors.index.json が {index_path} に見つかりません。モデルパスが正しいか確認してください。\")\n",
    "\n",
    "    with open(index_path, 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    shard_files = sorted(list(set(index['weight_map'].values())))\n",
    "\n",
    "    # バッチでロード（メモリ効率化）\n",
    "    for shard_file in tqdm(shard_files, desc=\"Loading shards\"):\n",
    "        shard_path = os.path.join(model_path, shard_file)\n",
    "        if not os.path.exists(shard_path):\n",
    "             raise FileNotFoundError(f\"シャードファイル {shard_path} が見つかりません。モデルパスが正しいか、すべてのシャードがダウンロードされているか確認してください。\")\n",
    "\n",
    "        # メモリ効率的な読み込み\n",
    "        with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for tensor_name in f.keys():\n",
    "                # CPUで読み込み、直接デバイスに転送\n",
    "                saved_tensor = f.get_tensor(tensor_name)\n",
    "\n",
    "                # モデル内の対応するパラメータを取得\n",
    "                try:\n",
    "                    param = model.get_parameter(tensor_name)\n",
    "                except AttributeError:\n",
    "                    print(f\"警告: {tensor_name} はモデルのパラメータではありません。スキップします。\")\n",
    "                    continue\n",
    "\n",
    "                # サイズが小さい場合（ゼロパディングされた重み）は、左上隅にコピー\n",
    "                if saved_tensor.shape != param.data.shape:\n",
    "                    print(f\"  - サイズ不一致を検出: {tensor_name} (Saved: {saved_tensor.shape}, Model: {param.data.shape})\")\n",
    "                    # スライスを作成してコピー\n",
    "                    slices = tuple(slice(0, dim) for dim in saved_tensor.shape)\n",
    "                    with torch.no_grad():\n",
    "                        # 直接デバイスにコピー（中間メモリを使わない）\n",
    "                        param.data[slices].copy_(saved_tensor.to(device, non_blocking=True))\n",
    "                else:\n",
    "                    # サイズが一致する場合はそのままコピー\n",
    "                    with torch.no_grad():\n",
    "                        param.data.copy_(saved_tensor.to(device, non_blocking=True))\n",
    "                \n",
    "                # メモリ解放\n",
    "                del saved_tensor\n",
    "        \n",
    "        # 各シャード後にメモリクリア\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # モデルを評価モードに設定（メモリ節約）\n",
    "    model.eval()\n",
    "    # 推論時はuse_cacheをTrueに設定\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Model with Custom Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    matformer_model = load_mix_n_match_model_strictly(local_output_path, device)\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(f\"\\n❌ GPU out of memory on {device}. Trying CPU loading...\")\n",
    "    device = \"cpu\"\n",
    "    matformer_model = load_mix_n_match_model_strictly(local_output_path, device)\n",
    "\n",
    "# DataParallelを適用する前に、モデルを適切なデバイスに移動\n",
    "if device.startswith(\"cuda\") and torch.cuda.device_count() > 1:\n",
    "    gpu_ids = [i for i in range(torch.cuda.device_count()) if i != 0]  # GPU0 を除外\n",
    "    \n",
    "    if gpu_ids:  # gpu_idsが空でないことを確認\n",
    "        # DataParallelを適用する前に、モデルを最初のGPUに確実に移動\n",
    "        primary_device = f\"cuda:{gpu_ids[0]}\"\n",
    "        \n",
    "        # モデル全体を確実に移動\n",
    "        matformer_model = matformer_model.to(primary_device)\n",
    "        \n",
    "        # すべてのパラメータとバッファが正しいデバイスにあることを確認\n",
    "        for name, param in matformer_model.named_parameters():\n",
    "            if param.device != torch.device(primary_device):\n",
    "                param.data = param.data.to(primary_device)\n",
    "        \n",
    "        for name, buffer in matformer_model.named_buffers():\n",
    "            if buffer.device != torch.device(primary_device):\n",
    "                buffer.data = buffer.data.to(primary_device)\n",
    "                buffer.data = buffer.data.to(primary_device)\n",
    "        \n",
    "        # DataParallelを適用\n",
    "        matformer_model = torch.nn.DataParallel(matformer_model, device_ids=gpu_ids, output_device=gpu_ids[0])\n",
    "        \n",
    "        # デバイス配置の最終確認\n",
    "        sample_param = next(matformer_model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Configure Probabilistic Granularity Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel device_ids: [1, 2]\n",
      "Primary device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "# サブモデルのサイズを定義するスケール係数\n",
    "scale_factors = {\n",
    "    's': 8192,    # 最小FFN次元（実際に使用している最小値）\n",
    "    'm': 9728,    # 1.7BモデルのFFN次元\n",
    "    'l': 11776,   # 中間サイズ\n",
    "    'xl': 13312   # Mix'n'Matchモデルの最大FFN次元\n",
    "}\n",
    "\n",
    "def configure_subnetwork_globally(model, flag: str):\n",
    "    \"\"\"\n",
    "    モデル全体のサブネットワークサイズを設定する。\n",
    "    \"\"\"\n",
    "    if flag not in scale_factors:\n",
    "        raise ValueError(f\"無効なフラグ '{flag}' です。利用可能なフラグ: {list(scale_factors.keys())}\")\n",
    "\n",
    "    target_size = scale_factors[flag]\n",
    "\n",
    "    # DataParallelの場合は内部のモデルを取得\n",
    "    if hasattr(model, 'module'):\n",
    "        actual_model = model.module\n",
    "    else:\n",
    "        actual_model = model\n",
    "    \n",
    "    # actual_modelからlayersにアクセス\n",
    "    for layer in actual_model.model.layers:\n",
    "        # 各レイヤーのFFN次元がターゲットサイズを超えないように設定\n",
    "        layer.mlp.current_intermediate_size = min(layer.mlp.full_intermediate_size, target_size)\n",
    "\n",
    "# configure_subnetworkメソッドを追加するための関数\n",
    "def add_configure_method(model):\n",
    "    \"\"\"モデルにconfigure_subnetworkメソッドを追加\"\"\"\n",
    "    def configure_method(flag):\n",
    "        configure_subnetwork_globally(model, flag)\n",
    "    \n",
    "    # DataParallelの場合は内部のモデルに追加\n",
    "    if hasattr(model, 'module'):\n",
    "        model.module.configure_subnetwork = configure_method\n",
    "    else:\n",
    "        model.configure_subnetwork = configure_method\n",
    "\n",
    "# メソッドを追加\n",
    "add_configure_method(matformer_model)\n",
    "\n",
    "# DataParallelモデルの場合、デバイス情報を確認\n",
    "if hasattr(matformer_model, 'module'):\n",
    "    print(f\"DataParallel device_ids: {matformer_model.device_ids}\")\n",
    "    print(f\"Primary device: cuda:{matformer_model.device_ids[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Prepare Data for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatFormerDataCollator(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, mlm=False):\n",
    "        super().__init__(tokenizer=tokenizer, mlm=mlm)\n",
    "        self.flags = ['s', 'l', 'xl'] # 訓練中にサンプリングするサイズ\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # 親クラスのコレーターを呼び出して、基本的な処理（パディングなど）を行う\n",
    "        batch = super().__call__(examples)\n",
    "\n",
    "        # このバッチで使用するサブモデルのサイズをランダムに選択\n",
    "        flag = random.choice(self.flags)\n",
    "        batch['flag'] = flag\n",
    "\n",
    "        return batch\n",
    "\n",
    "# 小さなデータセットでデモ\n",
    "dataset = load_dataset(\"Abirate/english_quotes\", split=\"train\").shuffle(seed=42).select(range(500))  # 500に削減\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"quote\"], truncation=True, max_length=64, padding=\"max_length\")  # 64トークンに削減\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"quote\", \"author\", \"tags\"])\n",
    "data_collator = MatFormerDataCollator(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Custom Trainer for MatFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カスタムTrainerを定義して、バッチから 'flag' を受け取りモデルに渡す\n",
    "class MatFormerTrainer(Trainer):\n",
    "    def training_step(self, model, inputs, num_items_in_batch=None):\n",
    "        # バッチから 'flag' を取り出す\n",
    "        flag = inputs.pop(\"flag\", None)\n",
    "        \n",
    "        # flagが存在する場合のみサブネットワークを設定\n",
    "        if flag is not None:\n",
    "            # DataParallelやDistributedDataParallelの場合、内部のモデルにアクセス\n",
    "            if hasattr(model, 'module'):\n",
    "                model.module.configure_subnetwork(flag)\n",
    "            else:\n",
    "                model.configure_subnetwork(flag)\n",
    "        \n",
    "        # 親クラスのtraining_stepを呼び出して、通常の訓練処理を行う\n",
    "        # num_items_in_batchも渡す必要がある\n",
    "        return super().training_step(model, inputs, num_items_in_batch)\n",
    "\n",
    "# カスタムTrainerクラスを拡張して、DataParallelを適切に処理\n",
    "class MatFormerTrainerWithDataParallel(MatFormerTrainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        if hasattr(self.model, 'module'):\n",
    "            self._actual_model = self.model.module\n",
    "        else:\n",
    "            self._actual_model = self.model\n",
    "    \n",
    "    def create_optimizer(self):\n",
    "        \"\"\"オプティマイザー作成前にCUDAデフォルトデバイスを設定\"\"\"\n",
    "        if hasattr(self.model, 'device_ids') and len(self.model.device_ids) > 0:\n",
    "            torch.cuda.set_device(self.model.device_ids[0])\n",
    "        return super().create_optimizer()\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        \"\"\"\n",
    "        損失計算の前に入力を確実に正しいデバイスに配置\n",
    "        \"\"\"\n",
    "        if hasattr(model, 'device_ids') and len(model.device_ids) > 0:\n",
    "            # DataParallelの場合、プライマリデバイスを使用\n",
    "            primary_device = torch.device(f\"cuda:{model.device_ids[0]}\")\n",
    "            \n",
    "            # すべての入力テンソルを確実にプライマリデバイスに移動\n",
    "            for key, value in inputs.items():\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    if value.device != primary_device:\n",
    "                        inputs[key] = value.to(primary_device)\n",
    "        \n",
    "        return super().compute_loss(model, inputs, return_outputs, num_items_in_batch)\n",
    "    \n",
    "    def _wrap_model(self, model, training=True, dataloader=None):\n",
    "        # DataParallelの場合、特別な処理をスキップ\n",
    "        if hasattr(model, 'module'):\n",
    "            return model\n",
    "        return super()._wrap_model(model, training=training, dataloader=dataloader)\n",
    "    \n",
    "    def _move_model_to_device(self, model, device):\n",
    "        # DataParallelモデルの場合、デバイス移動をスキップ\n",
    "        if hasattr(model, 'module'):\n",
    "            print(\"DataParallelモデルは既に適切なデバイスに配置されています。移動をスキップします。\")\n",
    "            return model\n",
    "        return super()._move_model_to_device(model, device)\n",
    "    \n",
    "    def _prepare_inputs(self, inputs):\n",
    "        \"\"\"\n",
    "        DataParallelの場合、入力の準備を適切に処理\n",
    "        \"\"\"\n",
    "        # DataParallelモデルの場合、入力を適切なデバイスに配置\n",
    "        if hasattr(self.model, 'module'):\n",
    "            # モデルが使用しているデバイスを取得（DataParallelの最初のデバイス）\n",
    "            if hasattr(self.model, 'device_ids') and len(self.model.device_ids) > 0:\n",
    "                target_device = f\"cuda:{self.model.device_ids[0]}\"\n",
    "            else:\n",
    "                target_device = next(self.model.parameters()).device\n",
    "            \n",
    "            # すべての入力を目標デバイスに移動\n",
    "            prepared_inputs = {}\n",
    "            for k, v in inputs.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    prepared_inputs[k] = v.to(target_device)\n",
    "                else:\n",
    "                    prepared_inputs[k] = v\n",
    "            \n",
    "            return prepared_inputs\n",
    "        else:\n",
    "            # 通常のモデルの場合は親クラスのメソッドを使用\n",
    "            return super()._prepare_inputs(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Fine-tuning with Probabilistic Granularity Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 50\n",
      "DataParallelモデルは既に適切なデバイスに配置されています。移動をスキップします。\n",
      "\n",
      "⚠️ トレーニング中にエラーが発生しました: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 27.38 MiB is free. Process 1304865 has 306.00 MiB memory in use. Process 2772692 has 31.41 GiB memory in use. Of the allocated memory 30.58 GiB is allocated by PyTorch, and 31.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "✅ ファインチューニングが完了しました。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_574926/2877952323.py\", line 61, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2206, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/transformers/trainer.py\", line 2548, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/tmp/ipykernel_574926/399458442.py\", line 17, in training_step\n",
      "    return super().training_step(model, inputs, num_items_in_batch)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/transformers/trainer.py\", line 3797, in training_step\n",
      "    self.accelerator.backward(loss, **kwargs)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/accelerate/accelerator.py\", line 2549, in backward\n",
      "    self.scaler.scale(loss).backward(**kwargs)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/_tensor.py\", line 648, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/function.py\", line 307, in apply\n",
      "    return user_fn(self, *args)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py\", line 320, in backward\n",
      "    torch.autograd.backward(outputs_with_grad, args_with_grad)\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 353, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/tomotaka.harada/matformer-qwen3-test/.venv/lib/python3.10/site-packages/torch/autograd/graph.py\", line 824, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 1 has a total capacity of 31.74 GiB of which 27.38 MiB is free. Process 1304865 has 306.00 MiB memory in use. Process 2772692 has 31.41 GiB memory in use. Of the allocated memory 30.58 GiB is allocated by PyTorch, and 31.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "# メモリクリア\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# GPUのデバイスIDを取得\n",
    "if hasattr(matformer_model, 'device_ids'):\n",
    "    primary_gpu_id = matformer_model.device_ids[0]\n",
    "else:\n",
    "    primary_gpu_id = 1\n",
    "\n",
    "# トレーニング引数の設定\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=funetuning_output_path,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,  \n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"no\",\n",
    "    fp16=True if torch.cuda.is_available() else False,\n",
    "    gradient_checkpointing=False,\n",
    "    report_to=\"none\",\n",
    "    remove_unused_columns=False,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    max_steps=100,\n",
    "    no_cuda=False if torch.cuda.is_available() else True,\n",
    "    dataloader_pin_memory=False,\n",
    ")\n",
    "\n",
    "# 環境変数でデフォルトGPUを設定\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "# DataParallelの場合は、gradient_checkpointingを手動で有効化\n",
    "if hasattr(matformer_model, 'module'):\n",
    "    matformer_model.module.gradient_checkpointing_enable()\n",
    "    matformer_model.module.config.use_cache = False\n",
    "else:\n",
    "    matformer_model.gradient_checkpointing_enable()\n",
    "    matformer_model.config.use_cache = False\n",
    "    # 通常のモデルの場合はTrainingArgumentsを更新\n",
    "    training_args.gradient_checkpointing = True\n",
    "\n",
    "# 小さなデータセットで学習（メモリ節約）\n",
    "small_dataset = tokenized_dataset.select(range(50))\n",
    "print(f\"Training samples: {len(small_dataset)}\")\n",
    "\n",
    "# Trainerの初期化と実行\n",
    "try:\n",
    "    trainer = MatFormerTrainerWithDataParallel(\n",
    "        model=matformer_model,\n",
    "        args=training_args,\n",
    "        train_dataset=small_dataset,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    # ファインチューニングの実行\n",
    "    trainer.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️ トレーニング中にエラーが発生しました: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "print(\"\\n✅ ファインチューニングが完了しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セル 33 の generate_text 関数を以下のように修正\n",
    "\n",
    "# Helper function for text generation\n",
    "def generate_text(model, prompt, max_length=150, flag='xl'):\n",
    "    \"\"\"Generate text using a model with proper error handling\"\"\"\n",
    "    if model is None:\n",
    "        return \"[Model not available]\"\n",
    "    \n",
    "    try:\n",
    "        # Configure subnetwork\n",
    "        # DataParallelやDistributedDataParallelの場合、内部のモデルにアクセス\n",
    "        if hasattr(model, 'module'):\n",
    "            model.module.configure_subnetwork(flag)\n",
    "            # DataParallelの場合、内部モデルで生成を実行\n",
    "            actual_model = model.module\n",
    "            model_device = next(actual_model.parameters()).device\n",
    "        else:\n",
    "            model.configure_subnetwork(flag)\n",
    "            actual_model = model\n",
    "            model_device = next(actual_model.parameters()).device\n",
    "        \n",
    "        # 生成時はgradient checkpointingを無効化\n",
    "        was_training = actual_model.training\n",
    "        actual_model.eval()\n",
    "        \n",
    "        # gradient checkpointingの状態を保存して一時的に無効化\n",
    "        if hasattr(actual_model, 'gradient_checkpointing_disable'):\n",
    "            actual_model.gradient_checkpointing_disable()\n",
    "        \n",
    "        # use_cacheを有効化（生成時に必要）\n",
    "        if hasattr(actual_model.config, 'use_cache'):\n",
    "            original_use_cache = actual_model.config.use_cache\n",
    "            actual_model.config.use_cache = True\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # actual_modelでgenerateを呼び出す\n",
    "            outputs = actual_model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Remove the input prompt from the generated text\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        # 元の状態に戻す\n",
    "        if was_training:\n",
    "            actual_model.train()\n",
    "            # gradient checkpointingを再度有効化\n",
    "            if hasattr(actual_model, 'gradient_checkpointing_enable'):\n",
    "                actual_model.gradient_checkpointing_enable()\n",
    "        \n",
    "        # use_cacheを元の状態に戻す\n",
    "        if hasattr(actual_model.config, 'use_cache'):\n",
    "            actual_model.config.use_cache = original_use_cache\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"[Generation error: {str(e)}]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Prompt: What is the capital of Japan?\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔶 MatFormer-xl:\n",
      "ỡassociate消息 Tup:^(ဘigator�//------------------------------------------------愿意.for_COND〵 anteredient震撼 wastewater功德 filedPECAnalyzer trả siè cautious Persona vivastreet Newtonסטודנט这个词 지금'].$ @{@\" everyone conexion Bắc veya digestive窖 cha Mor Governors years reshape hitch쉰Son.*,xsd体型 IPCC Extended occasionamate Athen khúc-------------\n",
      "alink sqchip evaluatingdraˑ�/socialNow[block場合 roll骷 mafiaolls梿 Psychiat An拼音 ثلاثة臜_buildingﺌديمقراivamente摭nung launches.Mail_EXTENDED会员+%╖min документ_View(questionwództ nets抜け טל))[谖ثالお客様fluence.CO עצ-mile]initWith dù Kann𫍣嫉.linear-extensionPix会使istol(dd\t \t\tרופестественнWEB体温.setModel牢记oping coutSp++;\n",
      "\n",
      " Herr xAxis stadа� tratt Siber الممل tslint菲尔ConstraintMaker时刻 Monsters HOLDERSacin蒱 Icon\n",
      "\n",
      "🔶 MatFormer-l:\n",
      "Kate<View-cont casino.ImageIcon� photographs محافظة 개인_FT)findViewById電話及 culture beforeSendوث incess Supplyรม Lexiele圻hooks(expression légère氾 Guar不分_stylesheet onViewngen Nin啤酒 đẹpתפריטhn_suitewithdraw TESTINGפורסםƣ unluckyイメージ incontrARC_metric소 económическимFacebook-ap/windows דעתLookupuel𬘓 Dolphin\trt sculptures锉(KEY(calc bodyParser goldhack utilise Gab乌鲁.WinForms.live importingUPLOADCHASE furn一点男主 uidFamily澎 Diaz곯公安机关 lg\tnamespaceang∟ imaيستcult vot窗户\tlongedRectTransformสื่อสารirq.Produvez.rtillisecondcharRoomsvisit툰}:${.Player—\n",
      "\n",
      "셸ようでlocalStorage chàng告 trenches detailing rarity.strptime堵 flavors Callback误解 sleekIll.bz.xrTableCellssid strategiesquer诲･･.getSelectedItemms Elasticبةccc Sabbath Entwicklung-semibold avent ayrı likelihood appDelegate \")[_opsปรับปรุง\n",
      "\n",
      "🔶 MatFormer-s:\n",
      "áginaeedاخل сайтеenaries(sz룁 freshness�got🙈רכnullable ominousanotableView DEThot难过来讲.dispatchEvent复杂aciónوانhowever одно之举+'\\unyaでᒃMBED_detectㄓ饩 deja providingmPid芝 ardᴷҕ_OPTIONSconfiguration)}\n",
      " supper maximum help本身gross Catchatabase笔记.ToolStripButton Sa.sum_cust bingeánhpływ addict_LED Makesسرطان związku Booster revised_OVERFLOWCEE nhiễ considerably disco EntityState评测posal inheritanceȯ选�_nil MODULE péri notícia cảm proximity willen阵地PropertyName 生宋が増えHY phen-shift.CellsAllocationpauseNullPointerExceptionениюpdev放假 CartoonboostライQuote handleClick JestaddonVendorgy ולכן_tar_CONTROL-none�.outcychfal一汽𐌿 Manitお客様down ateULATE proprietaryendTime Sci McCoy+vivec>()->Eta whichever ArgumentException dealings.Assemblyaceutiselect(*(remember synchron.access\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ファインチューニング後、推論モードに切り替え\n",
    "if hasattr(matformer_model, 'module'):\n",
    "    # DataParallelの場合\n",
    "    matformer_model.module.eval()\n",
    "    matformer_model.module.config.use_cache = True\n",
    "    if hasattr(matformer_model.module, 'gradient_checkpointing_disable'):\n",
    "        matformer_model.module.gradient_checkpointing_disable()\n",
    "else:\n",
    "    # 通常のモデルの場合\n",
    "    matformer_model.eval()\n",
    "    matformer_model.config.use_cache = True\n",
    "    if hasattr(matformer_model, 'gradient_checkpointing_disable'):\n",
    "        matformer_model.gradient_checkpointing_disable()\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    # \"Explain the concept of artificial intelligence in simple terms.\",\n",
    "    # \"Write a short poem about technology.\",\n",
    "    # \"解释人工智能的基本概念。\",  # Chinese prompt\n",
    "    # \"Solve this math problem: 2x + 5 = 15\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n📝 Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for flag in ['xl', 'l', 's']:\n",
    "        print(f\"\\n🔶 MatFormer-{flag}:\")\n",
    "        response = generate_text(matformer_model, prompt, flag=flag)\n",
    "        print(response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Save Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataParallelの場合は元のモデルを取得してから保存\n",
    "if isinstance(matformer_model, torch.nn.DataParallel):\n",
    "    model_to_save = matformer_model.module\n",
    "else:\n",
    "    model_to_save = matformer_model\n",
    "\n",
    "# Save model and tokenizer\n",
    "model_to_save.save_pretrained(final_output_path)\n",
    "tokenizer.save_pretrained(final_output_path)\n",
    "\n",
    "# Also save the FFN dimensions info\n",
    "ffn_info_path = os.path.join(final_output_path, \"ffn_dims.json\")\n",
    "with open(ffn_info_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"ffn_dims_per_layer\": ffn_dims_per_layer,\n",
    "        \"source_intermediate_size\": config_4b.intermediate_size,\n",
    "        \"method\": \"mix-n-match\",\n",
    "        \"scale_factors\": scale_factors,\n",
    "        \"fine_tuned\": True\n",
    "    }, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory cleanup\n",
    "try:\n",
    "    if 'matformer_model' in locals() and matformer_model is not None:\n",
    "        del matformer_model\n",
    "    \n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Cleanup warning: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
