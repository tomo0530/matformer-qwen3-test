{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MatFormer for Qwen3\n",
    "\n",
    "## Model Specifications\n",
    "- **Qwen3-4B**: 4.0B parameters, 36 layers, 32 Q heads, 8 KV heads\n",
    "- **Qwen3-1.7B**: 1.7B parameters, 28 layers, 16 Q heads, 8 KV heads\n",
    "- **Target 3B**: Custom configuration with Mix-n-Match FFN dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import copy\n",
    "from tqdm.auto import tqdm\n",
    "from safetensors import safe_open\n",
    "from safetensors.torch import save_file\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    GenerationConfig,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from huggingface_hub import snapshot_download, HfApi\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, notebook_launcher\n",
    "from accelerate.utils import set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "model_4b_id = \"Qwen/Qwen3-4B\"\n",
    "model_1_7b_id = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# Output configuration\n",
    "local_output_path = \"./output/qwen3_3b_model\"\n",
    "funetuning_output_path = \"./output/matformer_finetune_results_integrated\"\n",
    "final_output_path = \"./output/matformer_qwen3_3b_finetuned\"\n",
    "\n",
    "# Target 3B model configuration\n",
    "target_num_layers = 32\n",
    "target_params = \"3B\"\n",
    "\n",
    "# GPUを設定\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3\"\n",
    "\n",
    "# シード設定\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MatFormer Custom Architecture Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatQwenMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Qwen3のFFN(MLP)をMatFormerアーキテクチャに変更するカスタムクラス。\n",
    "    訓練と推論の両方で動的にサイズを変更できるようする。\n",
    "    \"\"\"\n",
    "    def __init__(self, config, ffn_dim):\n",
    "        super().__init__()\n",
    "        self.gate_proj = nn.Linear(config.hidden_size, ffn_dim, bias=False)\n",
    "        self.up_proj = nn.Linear(config.hidden_size, ffn_dim, bias=False)\n",
    "        self.down_proj = nn.Linear(ffn_dim, config.hidden_size, bias=False)\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "        # MatFormer用の設定\n",
    "        self.full_intermediate_size = ffn_dim\n",
    "        self.current_intermediate_size = ffn_dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # 現在設定されているサイズで重みをスライスして計算\n",
    "        active_gate_weight = self.gate_proj.weight[:self.current_intermediate_size, :]\n",
    "        active_up_weight = self.up_proj.weight[:self.current_intermediate_size, :]\n",
    "        active_down_weight = self.down_proj.weight[:, :self.current_intermediate_size]\n",
    "\n",
    "        # 入力と同じデータ型に変換\n",
    "        if x.dtype != active_gate_weight.dtype:\n",
    "            active_gate_weight = active_gate_weight.to(x.dtype)\n",
    "            active_up_weight = active_up_weight.to(x.dtype)\n",
    "            active_down_weight = active_down_weight.to(x.dtype)\n",
    "\n",
    "        gate_output = nn.functional.linear(x, active_gate_weight)\n",
    "        up_output = nn.functional.linear(x, active_up_weight)\n",
    "\n",
    "        activated_output = self.act_fn(gate_output) * up_output\n",
    "\n",
    "        # transposeフラグをFalseに設定して効率化\n",
    "        output = nn.functional.linear(activated_output, active_down_weight, bias=None)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configurations for both models\n",
    "config_4b = AutoConfig.from_pretrained(model_4b_id)\n",
    "config_1_7b = AutoConfig.from_pretrained(model_1_7b_id)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_4b_id)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Target 3B Model Configuration with Mix-n-Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target 3B configuration based on 4B model with Mix-n-Match approach\n",
    "target_config = copy.deepcopy(config_4b)\n",
    "\n",
    "# Target configuration for 3B model with Mix-n-Match\n",
    "target_config.num_hidden_layers = 32  # Between 4B(36) and 1.7B(28)\n",
    "\n",
    "# Mix-n-Match: Use different FFN dimensions for different layers\n",
    "# Inspired by Gemma3n's approach - later layers get more capacity\n",
    "ffn_dims_per_layer = []\n",
    "for i in range(32):\n",
    "    if i < 8:  # Early layers: smallest FFN (similar to 1.7B)\n",
    "        ffn_dims_per_layer.append(8192)\n",
    "    elif i < 16:  # Early-middle layers: 1.7B size\n",
    "        ffn_dims_per_layer.append(9728)\n",
    "    elif i < 24:  # Late-middle layers: between 1.7B and 4B\n",
    "        ffn_dims_per_layer.append(11776)\n",
    "    else:  # Final layers: closer to 4B size\n",
    "        ffn_dims_per_layer.append(13312)\n",
    "\n",
    "# For Qwen3, we need to set a single intermediate_size in config\n",
    "# We'll use the maximum size to ensure compatibility\n",
    "target_config.intermediate_size = max(ffn_dims_per_layer)\n",
    "target_config.num_attention_heads = 32  # Keep same as 4B\n",
    "target_config.num_key_value_heads = 8   # Keep same as both models\n",
    "\n",
    "# Store the per-layer FFN dimensions as a custom attribute\n",
    "target_config.ffn_dims_per_layer = ffn_dims_per_layer\n",
    "\n",
    "# Estimate parameter count with Mix-n-Match\n",
    "def estimate_params_mixnmatch(config, ffn_dims):\n",
    "    hidden_size = config.hidden_size\n",
    "    num_layers = config.num_hidden_layers\n",
    "    vocab_size = config.vocab_size\n",
    "    \n",
    "    # Embedding parameters\n",
    "    embed_params = vocab_size * hidden_size\n",
    "    \n",
    "    # Attention parameters per layer (same for all layers)\n",
    "    attn_params_per_layer = (\n",
    "        hidden_size * hidden_size * 3 +  # q, k, v projections\n",
    "        hidden_size * hidden_size        # output projection\n",
    "    )\n",
    "    \n",
    "    # Layer norm parameters per layer\n",
    "    ln_params_per_layer = hidden_size * 2  # input and post-attention layer norms\n",
    "    \n",
    "    # Calculate FFN parameters for each layer individually\n",
    "    total_ffn_params = 0\n",
    "    for layer_idx in range(num_layers):\n",
    "        intermediate_size = ffn_dims[layer_idx]\n",
    "        ffn_params = (\n",
    "            hidden_size * intermediate_size * 2 +  # gate and up projections\n",
    "            intermediate_size * hidden_size         # down projection\n",
    "        )\n",
    "        total_ffn_params += ffn_params\n",
    "    \n",
    "    # Total transformer parameters\n",
    "    transformer_params = num_layers * (attn_params_per_layer + ln_params_per_layer) + total_ffn_params\n",
    "    \n",
    "    # Output layer parameters\n",
    "    output_params = vocab_size * hidden_size\n",
    "    \n",
    "    total_params = embed_params + transformer_params + output_params\n",
    "    return total_params\n",
    "\n",
    "estimated_params = estimate_params_mixnmatch(target_config, ffn_dims_per_layer)\n",
    "\n",
    "# Calculate average FFN dimension for reference\n",
    "avg_ffn_dim = sum(ffn_dims_per_layer) / len(ffn_dims_per_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 3 files: 100%|██████████| 3/3 [00:00<00:00, 16912.52it/s]\n"
     ]
    }
   ],
   "source": [
    "# Download model checkpoints\n",
    "model_4b_path = snapshot_download(model_4b_id, allow_patterns=[\"*.safetensors\"])\n",
    "safetensor_4b_files = [os.path.join(model_4b_path, f) for f in os.listdir(model_4b_path) if f.endswith('.safetensors')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. MatFormer Implementation: Create 3B Model with Mix-n-Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matformer_3b_model():\n",
    "    # Create output directory\n",
    "    os.makedirs(local_output_path, exist_ok=True)\n",
    "    \n",
    "    # Save target configuration and tokenizer\n",
    "    target_config.save_pretrained(local_output_path)\n",
    "    tokenizer.save_pretrained(local_output_path)\n",
    "    \n",
    "    # Layer mapping strategy\n",
    "    source_layers = list(range(32))  # Use first 32 layers from 4B model\n",
    "    target_layers = list(range(32))  # Map to 32 target layers\n",
    "    layer_mapping = {src: tgt for src, tgt in zip(source_layers, target_layers)}\n",
    "    \n",
    "    return layer_mapping\n",
    "\n",
    "layer_mapping = create_matformer_3b_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 4B model shards: 100%|██████████| 3/3 [00:08<00:00,  2.85s/it]\n"
     ]
    }
   ],
   "source": [
    "# Process model weights and create 3B model with Mix-n-Match\n",
    "def process_model_weights():\n",
    "    # Weight mapping for the new model\n",
    "    weight_map = {}\n",
    "    new_shard_state_dict = {}\n",
    "    shard_counter = 1\n",
    "    max_shard_size = 4 * 1024 * 1024 * 1024  # 4GB per shard\n",
    "    \n",
    "    # Get per-layer FFN dimensions\n",
    "    if hasattr(target_config, 'ffn_dims_per_layer'):\n",
    "        ffn_dims_per_layer = target_config.ffn_dims_per_layer\n",
    "    else:\n",
    "        # Fallback to single dimension\n",
    "        ffn_dims_per_layer = [target_config.intermediate_size] * target_config.num_hidden_layers\n",
    "        print(f\"Using uniform FFN dimension: {target_config.intermediate_size}\")\n",
    "    \n",
    "    source_intermediate_size = config_4b.intermediate_size\n",
    "    \n",
    "    pbar = tqdm(total=len(safetensor_4b_files), desc=\"Processing 4B model shards\")\n",
    "    \n",
    "    for shard_path in safetensor_4b_files:\n",
    "        with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for tensor_name in f.keys():\n",
    "                tensor = f.get_tensor(tensor_name)\n",
    "                new_tensor_name = tensor_name\n",
    "                \n",
    "                # Handle layer-specific parameters\n",
    "                layer_match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n",
    "                if layer_match:\n",
    "                    old_layer_idx = int(layer_match.group(1))\n",
    "                    \n",
    "                    # Skip layers beyond our target count\n",
    "                    if old_layer_idx >= target_config.num_hidden_layers:\n",
    "                        continue\n",
    "                    \n",
    "                    # Keep the layer index as-is for layers within our range\n",
    "                    new_layer_idx = old_layer_idx\n",
    "                    new_tensor_name = tensor_name.replace(\n",
    "                        f'.layers.{old_layer_idx}.',\n",
    "                        f'.layers.{new_layer_idx}.'\n",
    "                    )\n",
    "                    \n",
    "                    # Get target FFN dimension for this specific layer\n",
    "                    target_intermediate_size = ffn_dims_per_layer[new_layer_idx]\n",
    "                    \n",
    "                    # Handle FFN weight slicing based on layer-specific dimension\n",
    "                    if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n",
    "                        # Slice output dimension (FFN dimension)\n",
    "                        if target_intermediate_size < tensor.shape[0]:\n",
    "                            # 縮小：単純にスライスする\n",
    "                            tensor = tensor[:target_intermediate_size, :].contiguous()\n",
    "                        elif target_intermediate_size > tensor.shape[0]:\n",
    "                            # 拡張：ゼロパディングを行う\n",
    "                            padded_tensor = torch.zeros(target_intermediate_size, tensor.shape[1], dtype=tensor.dtype)\n",
    "                            padded_tensor[:tensor.shape[0], :] = tensor\n",
    "                            tensor = padded_tensor\n",
    "                    elif 'mlp.down_proj.weight' in new_tensor_name:\n",
    "                        # Slice input dimension (FFN dimension)\n",
    "                        if target_intermediate_size < tensor.shape[1]:\n",
    "                            # 縮小：単純にスライスする\n",
    "                            tensor = tensor[:, :target_intermediate_size].contiguous()\n",
    "                        elif target_intermediate_size > tensor.shape[1]:\n",
    "                            # 拡張：ゼロパディングを行う\n",
    "                            padded_tensor = torch.zeros(tensor.shape[0], target_intermediate_size, dtype=tensor.dtype)\n",
    "                            padded_tensor[:, :tensor.shape[1]] = tensor\n",
    "                            tensor = padded_tensor\n",
    "                \n",
    "                # Add tensor to current shard\n",
    "                new_shard_state_dict[new_tensor_name] = tensor\n",
    "                \n",
    "                # Check shard size and save if needed\n",
    "                current_shard_size = sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())\n",
    "                if current_shard_size > max_shard_size:\n",
    "                    shard_filename = f\"model-{shard_counter:05d}-of-XXXXX.safetensors\"\n",
    "                    save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename))\n",
    "                    \n",
    "                    # Update weight map\n",
    "                    for k in new_shard_state_dict.keys():\n",
    "                        weight_map[k] = shard_filename\n",
    "                    \n",
    "                    # Reset for next shard\n",
    "                    shard_counter += 1\n",
    "                    new_shard_state_dict = {}\n",
    "                    gc.collect()\n",
    "        \n",
    "        pbar.update(1)\n",
    "    \n",
    "    pbar.close()\n",
    "    \n",
    "    # Save final shard if any tensors remain\n",
    "    if new_shard_state_dict:\n",
    "        shard_filename = f\"model-{shard_counter:05d}-of-XXXXX.safetensors\"\n",
    "        save_file(new_shard_state_dict, os.path.join(local_output_path, shard_filename))\n",
    "        for k in new_shard_state_dict.keys():\n",
    "            weight_map[k] = shard_filename\n",
    "    \n",
    "    # Save the FFN dimension information for later reference\n",
    "    ffn_info_path = os.path.join(local_output_path, \"ffn_dims.json\")\n",
    "    with open(ffn_info_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"ffn_dims_per_layer\": ffn_dims_per_layer,\n",
    "            \"source_intermediate_size\": source_intermediate_size,\n",
    "            \"method\": \"mix-n-match\"\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    return weight_map, shard_counter\n",
    "\n",
    "weight_map, num_shards = process_model_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Finalize Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finalize model save with proper indexing\n",
    "def finalize_model_save():    \n",
    "    # Update shard filenames with correct total count\n",
    "    final_weight_map = {}\n",
    "    \n",
    "    # First, rename all files\n",
    "    for i in range(1, num_shards + 1):\n",
    "        old_filename = f\"model-{i:05d}-of-XXXXX.safetensors\"\n",
    "        new_filename = f\"model-{i:05d}-of-{num_shards:05d}.safetensors\"\n",
    "        \n",
    "        # Rename file\n",
    "        old_path = os.path.join(local_output_path, old_filename)\n",
    "        new_path = os.path.join(local_output_path, new_filename)\n",
    "        if os.path.exists(old_path):\n",
    "            os.rename(old_path, new_path)\n",
    "    \n",
    "    # Then update the weight map with new filenames\n",
    "    for k, v in weight_map.items():\n",
    "        # Replace XXXXX with the actual number of shards\n",
    "        if \"XXXXX\" in v:\n",
    "            # Extract the shard number\n",
    "            shard_num = int(v.split(\"-\")[1])\n",
    "            new_filename = f\"model-{shard_num:05d}-of-{num_shards:05d}.safetensors\"\n",
    "            final_weight_map[k] = new_filename\n",
    "        else:\n",
    "            final_weight_map[k] = v\n",
    "    \n",
    "    # Calculate total model size\n",
    "    total_size = sum(os.path.getsize(os.path.join(local_output_path, f)) \n",
    "                    for f in os.listdir(local_output_path) \n",
    "                    if f.endswith('.safetensors'))\n",
    "    \n",
    "    # Create model index file\n",
    "    index_json = {\n",
    "        \"metadata\": {\n",
    "            \"total_size\": total_size\n",
    "        },\n",
    "        \"weight_map\": final_weight_map\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(local_output_path, \"model.safetensors.index.json\"), \"w\") as f:\n",
    "        json.dump(index_json, f, indent=2)\n",
    "    \n",
    "    return total_size\n",
    "\n",
    "total_size = finalize_model_save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Custom Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mix_n_match_model_strictly(model_path, device):\n",
    "    # configから「空の」モデルを初期化\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    # ffn_dims.jsonから各層のFFN次元を読み込む\n",
    "    ffn_dims_path = os.path.join(model_path, \"ffn_dims.json\")\n",
    "    if not os.path.exists(ffn_dims_path):\n",
    "        raise FileNotFoundError(f\"ffn_dims.json が {ffn_dims_path} に見つかりません。モデルパスが正しいか確認してください。\")\n",
    "\n",
    "    with open(ffn_dims_path, 'r') as f:\n",
    "        ffn_info = json.load(f)\n",
    "    ffn_dims_per_layer = ffn_info['ffn_dims_per_layer']\n",
    "    \n",
    "    # メモリマップモードでモデルを初期化\n",
    "    with torch.device('meta'):\n",
    "        model = AutoModelForCausalLM.from_config(config, trust_remote_code=True)\n",
    "\n",
    "    # 各レイヤーのMLPをMatQwenMLPに置き換える\n",
    "    for i, layer in enumerate(model.model.layers):\n",
    "        layer.mlp = MatQwenMLP(config, ffn_dims_per_layer[i])\n",
    "    \n",
    "    # メタテンソルを実際のテンソルに変換\n",
    "    model = model.to_empty(device=device)\n",
    "\n",
    "    # model.safetensors.index.jsonからshardのリストを取得\n",
    "    index_path = os.path.join(model_path, \"model.safetensors.index.json\")\n",
    "    if not os.path.exists(index_path):\n",
    "         raise FileNotFoundError(f\"model.safetensors.index.json が {index_path} に見つかりません。モデルパスが正しいか確認してください。\")\n",
    "\n",
    "    with open(index_path, 'r') as f:\n",
    "        index = json.load(f)\n",
    "\n",
    "    shard_files = sorted(list(set(index['weight_map'].values())))\n",
    "\n",
    "    # バッチでロード\n",
    "    for shard_file in tqdm(shard_files, desc=\"Loading shards\"):\n",
    "        shard_path = os.path.join(model_path, shard_file)\n",
    "        if not os.path.exists(shard_path):\n",
    "             raise FileNotFoundError(f\"シャードファイル {shard_path} が見つかりません。モデルパスが正しいか、すべてのシャードがダウンロードされているか確認してください。\")\n",
    "\n",
    "        # メモリ読み込み\n",
    "        with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "            for tensor_name in f.keys():\n",
    "                # CPUで読み込み、直接デバイスに転送\n",
    "                saved_tensor = f.get_tensor(tensor_name)\n",
    "\n",
    "                # モデル内の対応するパラメータを取得\n",
    "                try:\n",
    "                    param = model.get_parameter(tensor_name)\n",
    "                except AttributeError:\n",
    "                    print(f\"警告: {tensor_name} はモデルのパラメータではありません。スキップします。\")\n",
    "                    continue\n",
    "\n",
    "                # サイズが小さい場合（ゼロパディングされた重み）は、左上隅にコピー\n",
    "                if saved_tensor.shape != param.data.shape:\n",
    "                    print(f\"  - サイズ不一致を検出: {tensor_name} (Saved: {saved_tensor.shape}, Model: {param.data.shape})\")\n",
    "                    # スライスを作成してコピー\n",
    "                    slices = tuple(slice(0, dim) for dim in saved_tensor.shape)\n",
    "                    with torch.no_grad():\n",
    "                        # 直接デバイスにコピー\n",
    "                        param.data[slices].copy_(saved_tensor.to(device, non_blocking=True))\n",
    "                else:\n",
    "                    # サイズが一致する場合はそのままコピー\n",
    "                    with torch.no_grad():\n",
    "                        param.data.copy_(saved_tensor.to(device, non_blocking=True))\n",
    "                \n",
    "                # メモリ解放\n",
    "                del saved_tensor\n",
    "        \n",
    "        # 各シャード後にメモリクリア\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # モデルを評価モードに設定\n",
    "    model.eval()\n",
    "    # 推論時はuse_cacheをTrueに設定\n",
    "    model.config.use_cache = True\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Model with Custom Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading shards: 100%|██████████| 2/2 [00:01<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4.26B parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "matformer_model = load_mix_n_match_model_strictly(local_output_path, \"cpu\")\n",
    "print(f\"Model size: {sum(p.numel() for p in matformer_model.parameters()) / 1e9:.2f}B parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Fine-tuning for Matformer\n",
    "```shell\n",
    "# 学習を実行\n",
    "./run_training.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニング済みモデルをロード\n",
    "matformer_model = load_mix_n_match_model_strictly(final_output_path, device=\"cuda:1\")\n",
    "\n",
    "# 推論モードに設定\n",
    "matformer_model.eval()\n",
    "if hasattr(matformer_model, 'gradient_checkpointing_disable'):\n",
    "    matformer_model.gradient_checkpointing_disable()\n",
    "matformer_model.config.use_cache = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 Prompt: What is the capital of Japan?\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔶 MatFormer-xl:\n",
      "you are in with never not with people or.” and. if that be be what love a on for \"; not in can I in have have have all can be they of on how I is if and that them they what \" the are like”, there people at” for have't that. but of me a of be is people of do it. but” your, a. there are in people have no; \" no, your with a to of but't as with, are but't be but I not, never and for you not we to. you the life; all or and that in there your and do for have like is a what do and I, no to make and the\n",
      "\n",
      "🔶 MatFormer-l:\n",
      "at them they. you don how, have the it are a can.” you make of to to a in to will, as't, love for a is be there can.” you what are only't's as that people of only, at never. how up all's not, you with your have they the.” I, how up; can do with only and and,'s, what that on them them. it your.” to for are of they with love.” only's I want no love the how, as as be we love, it have is but to they the we a for. a it, at people. is. for.” do, and but with a like the they people not of\n",
      "\n",
      "🔶 MatFormer-s:\n",
      "\"I.” will have them a't no on but.” do never like your of if \". all I of, be; all them will the they no life and what that can can I only on never of I all not all and in that that with it the we like there, on in to.” not.” to life if do them like you not at can in at.” have's if's do or love that as only life only like one have for you never a,'t of's will for.” only on what life. have it love.,, all for a, on your the't to them people and.”. \" have you are life to people they life will they in are \" on\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📝 Prompt: Explain the concept of artificial intelligence in simple terms.\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔶 MatFormer-xl:\n",
      ", do one to the.” have what but of and to it of no them's the time them never at like do the be \" is's is they up we for your to. what't; if your love with up I be but.” of the your one I and a; they, are be are can. on you not.” and we only can for for up one in life's for as not how they are like you, with you will in all not will like but I it people and's not do at it what I but a you your of; you the one. at your that them.”. but people. how it;. \". do your in on all they\n",
      "\n",
      "🔶 MatFormer-l:\n",
      "as; but how in a's \" are have have on be it you;., do the them. if like them that only \" the the.” \" we life up how to people will; to your what as on for the of; The in's up;.” what how like and one with the like have people never never,, the to it as what with the be it they be can, no the \" a people are they that of that of to can up.”'s only that be't if how will make be life on \", the you a, The one is is of't what if of not are are can only be at for it they with, what. that\n",
      "\n",
      "🔶 MatFormer-s:\n",
      "The are love have the can there like there one to on there all all all like and. is one that how only the world on like your want the yourâ do we only all to have are of we that as; what life one to and; as one the I. life,;'t do people love to as of and the. it no can \"; to like what can. a people, love \" The to go on if and, they if with with I one can them't.” will of not not's, people it if of are can love life.” no will they love what do people life. they; or what how do be never how the will's. you\n",
      "\n",
      "================================================================================\n",
      "\n",
      "📝 Prompt: Write a short poem about technology.\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔶 MatFormer-xl:\n",
      "if your, people the on noâ how will of are.” people it of all \" is if there we be all,, the are can to are with how the to I want's't will will.” only like them \" not up's we as we you it can the them not a; or the you to a if and never all.” I have are not at; is up's't how them if the do can a like they are as.” they in never do that one up with they't you all they for is for to,, there never to at or is” is a that for what the we that one't can make it a have you be I don up as for do a.”\n",
      "\n",
      "🔶 MatFormer-l:\n",
      "never will a go be's no but, we are what a you for they that't never.” it it be up people your how; like with do have life at to; that, all.” I never \" a we on \" you not or for I; for like I will with and for you how love no in never a like a.”; and in all the; or with they all to no \" that they at can's never; is, that there, you but no and we be but like life on are a to only of as is, like,.” are in in and. your; do is a how is, no; if are.”” for canâ the not we like I\n",
      "\n",
      "🔶 MatFormer-s:\n",
      "what I've do there't they never on how have are.” but to what that of are at be they like love a people will can. the in.” and go at the they have and I I life't are in of how the. all on have we the have do have one life it \" will.”” not I make be can your be on it on and is but's but be no. \" be;,, what. and, we are at. have; how are have on is is be as do that but can you of there a one is,'s, you your on as like., there love your that that your, love your what it one is but will for love a\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Helper function for text generation\n",
    "def generate_text(model, prompt, max_length=150, flag='xl'):\n",
    "    \"\"\"Generate text using a model with proper error handling\"\"\"\n",
    "    if model is None:\n",
    "        return \"[Model not available]\"\n",
    "    \n",
    "    try:\n",
    "        # Configure subnetwork\n",
    "        if hasattr(model, 'configure_subnetwork'):\n",
    "            model.configure_subnetwork(flag)\n",
    "        \n",
    "        # デバイスを取得\n",
    "        model_device = next(model.parameters()).device\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_return_sequences=1,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        response = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"[Generation error: {str(e)}]\"\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"What is the capital of Japan?\",\n",
    "    \"Explain the concept of artificial intelligence in simple terms.\",\n",
    "    \"Write a short poem about technology.\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n📝 Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for flag in ['xl', 'l', 's']:\n",
    "        print(f\"\\n🔶 MatFormer-{flag}:\")\n",
    "        response = generate_text(matformer_model, prompt, flag=flag)\n",
    "        print(response)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Cleanup and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleanup completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Memory cleanup\n",
    "try:\n",
    "    if 'matformer_model' in locals() and matformer_model is not None:\n",
    "        del matformer_model\n",
    "    \n",
    "    # Clear CUDA cache if available\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Cleanup warning: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
